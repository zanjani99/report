\chapter{مرور تاریخچه}

\section{روش های مبتنی برساختار}
روش‌های خلاصه‌سازی مبتنی بر ساختار از نخستین رویکردهای توسعه‌یافته در حوزه خلاصه‌سازی متون هستند. این روش‌ها با بهره‌گیری از ویژگی‌های ساختاری متن ورودی، به تولید خلاصه‌های مختصر و منسجم می‌پردازند. در این رویکرد، اطلاعات مهم متن به ساختاری از پیش تعریف‌شده تخصیص داده می‌شود و خلاصه بر اساس این ساختار ایجاد می‌گردد. با پیشرفت فناوری و ظهور روش‌های نوین، مانند مدل‌های زبانی پیشرفته و تکنیک‌های یادگیری عمیق، کارایی و دقت در خلاصه‌سازی متون بهبود یافته است. با این حال، روش‌های مبتنی بر ساختار همچنان به‌عنوان پایه و اساس درک و پردازش متون مورد استفاده قرار می‌گیرند. در این بخش روش‌های مبتنی بر درخت 
\LTRfootnote{tree-based}
، مبتنی بر قالب\LTRfootnote{template-based}،
مبتنی بر هستان‌شناسی\LTRfootnote{ontology-based}
، عبارت مقدمه و بدنه\LTRfootnote{lead-and-body phrase}
، مبتنی بر گراف\LTRfootnote{graph-based }
و مبتنی بر قانون\LTRfootnote{rule-based}
مورد بررسی قرار می‌گیرد. 

\subsection{روش مبتنی بر درخت}
روش‌های مبتنی بر درخت در خلاصه‌سازی متن از درخت‌های وابستگی برای نمایش ساختار نحوی اسناد متنی استفاده می‌کنند. ابتدا، متن مبدأ به درخت‌های وابستگی تبدیل می‌شود و سپس این درخت‌ها در یک ساختار واحد ادغام می‌شوند. در نهایت، با خطی‌سازی درخت ادغام‌شده، جملات جدیدی تولید می‌شوند. این فرآیند، که به آن «خطی‌سازی درخت» گفته می‌شود، به انتخاب تجزیه‌کننده و حفظ وابستگی‌های نحوی بین کلمات وابسته است که می‌تواند بر کارایی تأثیر بگذارد\cite{andhale2016overview}.
% برای بهبود این روش، تکنیک‌هایی مانند استفاده از تجزیه‌کننده‌های کم‌عمق برای ترکیب جملات مشابه، فشرده‌سازی از طریق حذف زیردرخت‌ها، تولید درخت‌های تودرتو با بهره‌گیری از ساختارهای بلاغی و تجزیه وابستگی پیشنهاد شده‌اند.
\subsection{روش مبتنی بر قالب}
این روش‌ها از قالب‌های از پیش تعریف‌شده برای نمایش اسناد استفاده می‌کنند. این قالب‌ها برای انطباق با الگوها و قوانین خاص در محتوای متنی طراحی شده‌اند و امکان استخراج اطلاعات مرتبط را فراهم می‌کنند که می‌توان آن‌ها را در چارچوب این قالب‌ها ترسیم کرد. این فرآیند شامل تطبیق متن با الگوها و قوانین مذکور برای شناسایی محتوای متناسب با الگو است. این روش به دلیل تولید خلاصه‌هایی که به ساختار و قالب‌های تعیین‌شده پایبند هستند، از انسجام بالایی برخوردار است. \cite{andhale2016overview}.
\subsection{روش مبتنی بر هستان‌شناسی}
روش‌های مبتنی بر هستی‌شناسی در خلاصه‌سازی متن از پایگاه‌های دانش برای بهبود فرآیند خلاصه‌سازی استفاده می‌کنند. بسیاری از اسناد اینترنتی به حوزه‌های خاص با واژگان محدود مرتبط هستند که می‌توان آن‌ها را با هستی‌شناسی‌ها بهتر نمایش داد. هستی‌شناسی‌ها نام‌گذاری و تعریف رسمی انواع موجودیت‌های یک دامنه خاص را ارائه می‌دهند و به‌عنوان پایگاه دانش عمل می‌کنند. با استفاده از هستی‌شناسی، سیستم خلاصه‌سازی می‌تواند نمایش معنایی محتوای اطلاعات را بهبود بخشد. تکنیک‌های این روش شامل ساخت مدل معنایی با هستی‌شناسی، نگاشت جملات به گره‌های آن و محاسبه امتیاز هر موجودیت برای رتبه‌بندی جملات است.
\cite{andhale2016overview}.
لی و همکاران  یک سیستم فازی را ارائه کرد که از هستی‌شناسی طراحی شده توسط متخصص حوزه اخبار استفاده می‌کند. جملات بر اساس طبقه‌بندی کننده اصطلاحی مبتنی بر هستی شناسی طبقه بندی می‌شوند. مکانیزم استنتاج فازی درجه عضویت برای هر جمله را با توجه به طبقه‌بندی کننده محاسبه می‌کند
\cite{lee2005fuzzy}.

\subsection{روش  عبارت مقدمه و بدنه}

روش «عبارت مقدمه و بدنه» در خلاصه‌سازی متن بر شناسایی و بازنگری جملات اصلی، معروف به جملات کلیدی، در یک سند تمرکز دارد. این جملات معمولاً حاوی اطلاعات مفید هستند و خلاصه‌ای جامع از محتوا ارائه می‌دهند. این روش شامل درج و جایگزینی عبارات در جملات اصلی برای ایجاد تکرار مناسب با بازبینی‌های معنایی است. از محدودیت‌های این روش می‌توان به نبود مدل تعمیم‌یافته برای خلاصه‌سازی و تأثیر منفی مدل‌های تجزیه دستوری اشاره کرد.
%این روش شامل درج و جایگزینی عبارات در جمله اصلی برای ایجاد تجدید نظرهای معنایی مناسب است. با بازنویسی تکراری جمله اصلی، جملات خلاصه جدیدی تولید می‌شوند. با این حال، یکی از محدودیت‌های این روش این است که تجزیه می‌تواند عملکرد آن را کاهش دهد، و هیچ مدل تعمیم یافته ای برای خلاصه‌سازی وجود ندارد
\cite{andhale2016overview}.
ایشیکاوا و همکاران  روش خلاصه‌سازی ترکیبی مبتنی بر روش فرکانس تکرار عبارت
\LTRfootnote{Term frequency (TF) }
و عبارت مقدمه و بدنه پیشنهاد کردند. تابع توزیع زاویه‌ای ضربدر بسامد عبارت  میزان اهمیت هر جمله را مشخص می‌کند. دستورها براساس اهمیت برای نوشتن خلاصه رتبه بندی می‌شوند
\cite{Ishikawa2001HybridTS}.

\subsection{روش مبتنی بر گراف}
یکی دیگر از رویکردهای خلاصه‌سازی، روش مبتنی بر گراف است که در آن هر جمله‌ی سند به‌عنوان یک گره در گراف نمایش داده می‌شود. جملات بر اساس روابط معنایی با یال‌ها به یکدیگر متصل می‌شوند و وزن یال‌ها نشان‌دهندهٔ قدرت این روابط است. سپس، با استفاده از الگوریتم‌های رتبه‌بندی گراف، اهمیت هر جمله تعیین می‌شود و جملات با اهمیت بالاتر در خلاصه گنجانده می‌شوند. این روش بدون نیاز به دانش عمیق زبانی یا حوزه‌ای، می‌تواند با انتخاب جملات مهم، خلاصه‌های مختصر و منسجمی ایجاد کند.
\cite{andhale2016overview}.
مالیروس و اسکیانیس  از مرکزیت گره برای نشان دادن اهمیت یک اصطلاح در سند استفاده می‌کنند. مرکزیت‌های گره محلی و جهانی برای وزن‌دهی عبارت در نظر گرفته می‌شوند تا خلاصه را شکل دهند
\cite{GraphBased}.
\subsection{روش مبتنی بر قانون }


در روش خلاصه‌سازی مبتنی بر قاعده، اسناد به دسته‌ها و جنبه‌های مختلف تقسیم می‌شوند. سپس، ماژول انتخاب محتوا بر اساس قوانین از پیش تعریف‌شده، اطلاعات بهینه را برای هر جنبه انتخاب می‌کند. در نهایت، با استفاده از الگوهای تولید، جملات خلاصه و مختصر ایجاد می‌شوند. به‌عبارت‌دیگر، این روش با بهره‌گیری از قوانین مشخص، مهم‌ترین اطلاعات مرتبط با هر جنبه را انتخاب کرده و سپس آن‌ها را در قالب یک خلاصه منسجم ارائه می‌دهد\cite{Moratanchsurvey}.
%\todo{edit}

%در این تکنیک، اسنادی که باید خلاصه شوند در قالب کلاس‌ها و فهرستی از جنبه ها به تصویر کشیده می‌شوند. ماژول انتخاب محتوا، موثرترین گزینه را در میان گزینه‌های تولیدشده توسط قوانین استخراج داده انتخاب می‌کند تا به یک یا تعداد زیادی از جنبه های یک دسته پاسخ دهد. در نهایت، الگوهای تولید برای تولید جملات طرح کلی استفاده می‌شوند.

%\todo{مقدمه؟}

% هدف خلاصه‌سازی انتزاعی متن تبدیل یک دنباله از کلمات به دنباله‌ای دیگر از کلمات است. پژوهش‌ها نشان داده است که مدل‌‌های دنباله به دنباله\LTRfootnote{sequence to sequence} 
% با استفاده از معماری کدگذار کدگشا\LTRfootnote{encoder decoder}
  %بهترین انتخاب برای مدل‌سازی وظایف تولید متن هستند. با این حال استفاده از آن‌ها ممکن است منجر به ایجاد خلاصه‌های فاقد اطلاعات مهم با چندین موضوع یا حاوی عبارات تکراری شود. در این بخش ابتدا  پژوهش‌های انجام شده در زمینه‌ی رفع مشکلات خلاصه‌سازی انتزاعی متن با استفاده از مدل‌های کدگذار-کدگشا و ترنسفورمر  \LTRfootnote{transformer}%   بررسی می‌شود.   سپس به ایده‌های ارائه شده برای حل چالش‌های محاسباتی مرتبط با پردازش دنباله‌های طولانی پرداخته ‌می‌شود. 

% 
روش‌های سنتی خلاصه‌سازی متن، هرچند در زمان خود مؤثر بوده‌اند، اما در مقایسه با شبکه‌های عصبی مدرن کارایی کمتری دارند. این روش‌ها غالباً به دانش زبانی عمیق و قوانین از پیش تعریف‌شده متکی هستند که ممکن است در مواجهه با متون متنوع و پیچیده ناکارآمد باشند.




%\subsection{روش‌های ارايه شده برای متون کوتاه }


%\subsection{روش‌های ارايه شده برای متون طولانی }


\section{روش‌های مبتنی بر مدل ترنسفورمر‌}
با ظهور ترنسفورمرها\LTRfootnote{transformers}
، بهبودهای قابل توجهی در کیفیت نتایج خلاصه‌سازی خودکار به وجود آمد. ترنسفومرها با استفاده از مکانیزم توجه به خود\LTRfootnote{Self-attention}
شباهت بین ورودی‌ها را بدون توجه به موقعیت موازی آن‌ها با حضور مستقل هر توکن در توالی ورودی مدل می‌کنند و به طور مؤثر مشکلات شبکه‌های بازگشتی را حل می‌کنند \cite{vaswani2017attention}. یکی از جهت‌گیری‌های رایج پژوهشی، اصلاح یا تطبیق ترنسفورمرها و مدل‌های زبانی از پیش آموزش دیده با وظایف مختلف مانند خلاصه‌سازی است. مدل‌های مبتنی بر مدل‌های زبانی از پیش آموزش دیده که با هدف خلاصه‌سازی انتزاعی طراحی شده‌اند از ویژگی‌های معنایی و متنی غنی بازنمایی‌های زبان برای بهبود کیفیت و دقت خلاصه‌‌ها استفاده می‌کنند.



پان
\LTRfootnote{Pan}
و همکاران یک مدل خلاصه‌سازی بر اساس مدل برت را پیشنهاد کرده‌اند. نویسندگان استدلال می‌کنند که خلاصه‌های تولید شده توسط مدل‌های خلاصه‌سازی متن موجود که موضوع متن را در نظر نمی‌گیرند، مرتبط یا حاوی اطلاعات مفید نیستند. 
مدل ارائه شده که تی‌برت‌سام\LTRfootnote{T-BERTSum}
نامیده می‌شود از سه بخش ایجاد بازنمایی، مدل موضوعی عصبی\LTRfootnote{Neural Topic Model(NTM)}
و مدل خلاصه‌سازی تشکیل شده است. ساختار مدل را در شکل \ref{fig:tBert_model} نشان داده شده است.
\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=10cm]{tbertsum_framework.png}
	\end{center}
	\caption{معماری مدل تی‌برت‌سام \cite{Ma2022TBERTSumTT}}
	\label{fig:tBert_model}
	\medskip
	\small
\end{figure}



همانطور که در شکل \ref{fig:tBert_embded} نشان داده شده است، بازنمایی ایجاد شده برای هر جمله ورودی، با استفاده از یک شبکه‌ی ترنسفورمر دوسویه‌ی\LTRfootnote{bidirectional}
چند لایه و حاصل جمع چهار نوع تعبیه ( تعبیه نشانه\LTRfootnote{token embedding}
، تعبیه قطعه\LTRfootnote{segment embedding}
، تعبیه موقعیت و تعبیه موضوع) به دست ‌می‌آید که تعبیه موضوع در این مقاله معرفی شده و سه تعبیه دیگر مشابه مدل برت هستند. وجود تعبیه موضوع در تولید بازنمایی هر کلمه یا جمله موجب افزودن اطلاعات پیش زمینه‌ای به هر کلمه و حل مشکل چند معنایی می‌شود.
\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=7cm]{TbertSum_embedding.png}
	\end{center}
	\caption{ تعبیه مدل تی‌برت‌سام \cite{Ma2022TBERTSumTT}}
	\label{fig:tBert_embded}
	\medskip
	\small
\end{figure}

مدل موضوعی عصبی وظیفه‌ی ایجاد تعبیه موضوعی را دارد. این مدل دارای دو جزء است: یک شبکه مولد و یک شبکه استنتاج.
شبکه مولد یک سند را به عنوان ورودی می‌گیرد و یک توزیع موضوعی را بر روی کلمات موجود در سند خروجی می‌دهد.
%\todo{مفهوم درست نیست}
شبکه استنتاج یک سند را به عنوان ورودی می‌گیرد و خروجی آن پارامترهای توزیع موضوع است. 
بخش خلاصه‌سازی مدل مبتنی بر معماری کدگذار - کدگشای ترنسفورمر است. کدگذار بازنمایی ایجاد شده را به عنوان ورودی می‌گیرد و دنباله‌ای از حالت‌های پنهان را تولید می‌کند. سپس کدگشا با استفاده از  حالت‌های پنهان و متن خلاصه را تولید می‌کند. همانطور که در شکل 
\ref{fig:tBert_transformer}
نشان داده شده است، به منظور فیلتر کردن اطلاعات کلیدی توالی ورودی، شبکه دروازه‌ای قبل از کدگشا اضافه می‌شود.
این شبکه برای کنترل جریان اطلاعات از دنباله ورودی به دنباله خروجی افزوده شده است و باعث می‌شود کدگشا بر روی تولید خلاصه از اطلاعات کلیدی و حذف اطلاعات غیرضروری تمرکز کند. این مدل می‌تواند خلاصه‌هایی تولید کند که مرتبط با موضوع متن و حاوی اطلاعات مفید باشد و قابلیت تطبیق با حوزه‌های مختلف را دارد.
%\todo{جمله‌ی اخر تکراری هست}

\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=10cm]{tbertsum_transformer.png}
	\end{center}
	\caption{ معماری ترنسفورمر تی‌برت‌سام \cite{Ma2022TBERTSumTT}}
	\label{fig:tBert_transformer}
	\medskip
	\small
	\centerline{	این مدل شامل شبکه‌ی دروازه‌ای و کدگذار-کدگشا با توجه چند سر می‌باشد \cite{Ma2022TBERTSumTT}}
	
\end{figure}
%\todo{ادیت}

اکثر مدل‌های خلاصه‌سازی انتزاعی موجود برای تولید خلاصه‌های با طول ثابت طراحی شده‌اند، بنابراین سو
\LTRfootnote{Ming-Hsiang Su}
و همکاران یک مدل دو مرحله‌ای مبتنی بر تنرسفورمر ارائه دادند که خلاصه‌های انتزاعی با طول متغیر را با توجه به تقاضای کاربر تولید کند. مطابق شکل  \ref{fig:two_stage_model}مدل پیشنهادی با تقسیم متن ورودی به بخش‌ها ، استخراج اطلاعات کلیدی و تولید خلاصه‌ی هر بخش، خلاصه‌ی انتزاعی با طول متغیر تولید می‌کند \cite{twostage}. بخش‌‌های مدل ارائه شده به شرح زیر است.
\begin{itemize}
	\item {
		بخش‌بندی متن: این مرحله متن ورودی را به تعدادی قسمت از پیش تعیین شده تقسیم می‌کند. تعداد بخش‌ها را می‌تواند توسط کاربر مشخص شود یا با توجه به نسبت دلخواه طول ورودی تنظیم کرد.
		برای شناسایی مرزهای بین بخش‌ها از مدل $ BERT-biLSTM $ استفاده می‌شود. این مرحله تضمین می‌کند که مرحله خلاصه‌سازی انتزاعی بر روی بخش‌های منسجم متن انجام می‌شود. هدف این بخش یافتن نقاط تقسیم‌بندی است که نشان دهنده تغییر موضوع در متن است و  به بهبود کیفیت خلاصه‌های تولید شده کمک می‌کند. 
	}
	\item {
		خلاصه‌سازی استخراجی: پس از تقسیم‌بندی متن، با استفاده از یک مدل خلاصه‌سازی استخراجی مبتنی بر برت‌سام
		\LTRfootnote{BertSum}
		مهم‌ترین جمله را از هر بخش استخراج می‌شود.
	}
	\item {
	خلاصه‌سازی اسناد: با استفاده از جملات استخراج شده این ماژول خلاصه سرفصل سند ورودی را تولید می‌کند که این خلاصه  به عنوان خروجی هدف در مرحله آموزش مدل دو مرحله‌ای استفاده می‌شود. مدل ترنسفورمر سند به حل مشکل تغییر طول ورودی و خروجی در کار خلاصه‌سازی کمک می‌کند.
	}
	\item{
	خلاصه‌سازی بخش: این ماژول وظیفه‌ی تولید خلاصه برای بخش‌های به دست آمده از مرحله تقسیم‌بندی متن را دارد.
}
\item{
آموزش مشارکتی:  برای آموزش متناوب ماژول خلاصه‌سازی بخش و ماژول خلاصه‌سازی اسناد تا زمان همگرایی آموزش مشارکتی اعمال می‌شود. این فرآیند به بهینه سازی عملکرد هر دو ماژول کمک می‌کند.}
\item{
	ایجاد خلاصه با طول متغیر:پس از اینکه متن ورودی به بخش‌های مختلف تقسیم شد، هر بخش از ماژول خلاصه‌سازی بخش عبور می‌کند تا یک خلاصه انتزاعی مبتنی بر جمله ایجاد کند. سپس این خلاصه‌های مبتنی بر جمله به هم متصل می‌شوند تا خلاصه انتزاعی با طول متغیر را تشکیل دهند. این فرآیند الحاق تضمین می‌کند که خلاصه تولید شده شامل اطلاعات تمام بخش های متن ورودی است.}
	
\end{itemize}
با ترکیب روش‌های استخراجی و انتزاعی در مدل خلاصه‌سازی دو مرحله‌ای، رویکرد پیشنهادی می‌تواند خلاصه‌های انتزاعی روان و با طول متغیر را با توجه به خواسته‌های کاربر تولید کند \cite{twostage}.




%خلاصه‌سازی اسناد: در مرحله دوم از جملات استخراج شده برای آموزش ماژول خلاصه‌سازی اسناد استفاده می‌شود. این ماژول یک خلاصه سرفصل از کل ورودی متن ایجاد می‌کند. پارامترهای این ماژول با در نظر گرفتن امتیازات ضرر ماژول خلاصه‌سازی اسناد و ماژول خلاصه‌سازی بخش به روز می‌شود.

%خلاصه‌سازی بخش‌ها: بخش‌های به دست آمده از مرحله تقسیم‌بندی متن برای آموزش ماژول خلاصه‌سازی در مرحله اول استفاده می‌شود. این ماژول یک خلاصه بر اساس جمله برای هر بخش تولید می‌کند. امتیازات ضرر ماژول خلاصه‌سازی سند و ماژول خلاصه‌سازی بخش برای به روز رسانی پارامترهای ماژول خلاصه‌سازی بخش در نظر گرفته می‌شود.



%خلاصه‌سازی با طول متغیر: در طول آزمایش، خروجی‌های ماژول خلاصه‌سازی بخش به هم متصل می‌شوند تا نتیجه خلاصه‌سازی انتزاعی با طول متغیر ارائه شود. تعداد بخش‌ها را می‌توان توسط کاربر مشخص کرد یا با توجه به نسبت دلخواه طول ورودی تنظیم کرد.

%با ترکیب روش‌های استخراجی و انتزاعی در مدل خلاصه‌سازی دو مرحله‌ای، رویکرد پیشنهادی می‌تواند خلاصه‌های انتزاعی روان و با طول متغیر را با توجه به خواسته‌های کاربر تولید کند \cite{twostage}.
%\todo{ ادیتش کن}
%این مدل می‌تواند خلاصه‌های انتزاعی با طول متغیر را با توجه به خواسته‌های کاربر ایجاد کند. این یک پیشرفت نسبت به مدل‌های قبلی است زیرا می‌تواند به طور همزمان به خلاصه‌سازی انتزاعی روان و با طول متغیر دست یابد.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=10cm]{two-stage model.png}
	\end{center}
	\caption{ چهارچوب ایجاد خلاصه با طول متغیر \cite{twostage}}
	\label{fig:two_stage_model}
	\medskip
	\small	
\end{figure}




لوئيس و همكاران مدلي با نام بارت 
\LTRfootnote{BART}
ارائه دادند. اين مدل مشابه با مدل اصلي ترنسفورمر، ساختاري كدگذار-کدگشا دارد. بر خلاف سادگي، به دليل داشتن كدگذار دو طرفه و كدگشاي چپ به راست اين مدل را مي‌توان نسخه عمومي‌تري از برت و جي‌پي‌تي 
\LTRfootnote{GPT}
 دانست. بارت در عمليات توليد متن، مانند ترجمه ماشيني يا خالصه‌سازي انتزاعي متن، و همچنين در فهم متن كاربرد دارد و با استفاده از اهداف کدگذاری خودکار حذف نویز آموزش می‌بیند. برای پیش‌آموزش بارت نواقصی در سندهای ورودی ایجاد می‌شود و با بهینه کردن تابع زیان آنتروپی-متقاطع \LTRfootnote{‫‪cross-entropy‬‬}
  بین خروجی‌های کدگشا و سند اولیه، متن بازسازی می‌شود. همانطور که در شکل\ref{fig:bart} نشان داده شده است این مدل طیف گسترده‌ای از نویز ‌ها از جمله پوشاندن توکن، حذف توکن، پر کردن متن، چرخش سند، به هم ریختن جمله (به هم زدن تصادفی ترتیب کلمه یک جمله) را استفاده  می‌کند \cite{lewis-etal-2020-bart}. 
%\todo{یک کم دیگه اضافه کن}
\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=4cm]{bart2.png}
	\end{center}
	\caption{ عمل‌های پیش‌آموزش بارت \cite{lewis-etal-2020-bart}}
	\label{fig:bart}
	\medskip
	\small
	%ورودی‌های کدگذار نیازی به هم‌سویی با خروجی‌های کدگشا ندارند، که امکان تبدیل نویز دلخواه را فراهم می‌کند. در اینجا، یک سند با جایگزین کردن دهانه‌های متن با نمادهای ماسک خراب شده است. سند خراب (سمت چپ) با یک مدل دو طرفه کدگذاری می‌شود و سپس احتمال سند اصلی (سمت راست) با کدگشای خودبازگشتی محاسبه می‌شود. برای تنظیم دقیق، یک سند خراب به رمزگذار و رمزگشا وارد می‌شود و ما از نمایش‌هایی از حالت پنهان‌ نهایی کدگشا استفاده می‌کنیم \cite{lewis-etal-2020-bart}.
\end{figure}


با اين كه بارت دقت خلاصه‌سازی انتزاعي متن را بهبود بخشيد، ولي مراحل پيش‌آموزش آن، مختص خلاصه‌سازی انتزاعي متن نیستند، در نتیجه در سال 2020 مدلي تحت عنوان پگاسوس\LTRfootnote{PEGASUS}
توسط ژنگ و همكاران ارائه شد كه معماري مشابه با بارت داشت ولي پيش‌آموزش آن مختص خلاصه‌سازی انتزاعي متن بود.
مدل پگاسوس
یک مدل دنباله به دنباله کدگذار کدگشا مبتنی بر ترنسفورمر است که بر روی مجموعه‌های متنی بدون نظارت با هدف تولید جملات فاصله‌افتاده\LTRfootnote{‫‪gap‬‬ ‫‪sentences‬‬ ‫‪generation‬‬}
از قبل آموزش داده شده است \cite{zhang2020pegasus}.

این مدل دو روش پيش‌آموزش را معرفي كرده است كه در ادامه به شرح آنها مي‌پردازيم:
\begin{enumerate}
	\item {
		توليد جملات فاصله‌افتاده : فرضی مطرح شده است كه اگر عمل پيش‌آموزش مدل به وظایف
		پايين‌دست\LTRfootnote{downstream task}
		نزديك‌تر باشد، نتیجه نهايي بهتر و همچنين تنظیم دقیق پارامترها
		\LTRfootnote{fine-tuning}
		سريع‌تر خواهد بود. با توجه به اين كه اين مدل قرار است فقط براي خلاصه‌سازی انتزاعي متن استفاده شود، عمل
		پيش‌آموزش مشابه توليد متن‌هاي خلاصه از يك سند ورودي تعریف شده‌است. تعدادي از جملات انتخاب شده و هر جمله
		به طور كامل با توكن $ [MASK1] $ جايگزين ميشود. براي انتخاب اين جملات، سه راه پيشنهاد
		شده است.
		\begin{itemize}
			\item {
				انتخاب تصادفي: $ m $جمله به صورت تصادفي از متن انتخاب شده و پنهان مي‌شوند.
			}
			\item{
				انتخاب جملات اول متن: $ m $جمله اول متن پنهان مي‌شوند زیرا اغلب جملات ابتداي متن نسبت به جملات بعدی مهم‌ترهستند.
			}
			\item{
				انتخاب جملات مهم متن: براي انتخاب $ m $ جمله مهم متن از تقريب معيار ارزيابي روژ-۱
				استفاده می‌شود. به ازای هر
				جمله از متن، يك دوتايي از آن جمله و متن سند فاقد آن جمله ساخته شده و ارزيابي
				مي‌شود كه چقدر ممکن است اين جمله، خلاصه سند فاقد آن جمله باشد. جملاتي
				كه امتیاز بالاتر گرفته‌اند از نظر خلاصه بودن مهم‌تر هستند و پنهان می‌شوند.
			}
			
			
		\end{itemize}
	}
	\item{
		 مدل زباني پوشيده شده: مشابه مدل برت ۵۱ درصد از توكن‌هاي متن ورودي انتخاب
		مي‌شوند و سپس 80 درصد از اين توكن‌ها، با توكن $ [MASK2] $ و10 درصد توكن‌ها با يك توكن
		تصادفي جايگزين می‌شوند. 10 درصد ديگر بدون تغيير باقي می‌ماند.
		شكل \ref{fig:pegasus} اعمال همزمان اين دو عمل، يعني توليد جمالت فاصله افتاده و مدل زباني پوشيده شده
		را بر روي يك ورودی نشان می‌دهد.
	}
\end{enumerate}

\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=5cm]{pegasus.png}
	\end{center}
	\caption{ ساختار مدل پگاسوس \cite{zhang2020pegasus}}
	\label{fig:pegasus}
	\medskip
	\small{
		معماری پایه پگاسوس یک کدگذار-کدگشا ترنسفورمر استاندارد است. جملات فاصله‌افتاده و مدل زبانی پوشیده شده به طور همزمان در این مثال به عنوان اهداف پيش‌آموزش اعمال می‌شوند. در اصل سه جمله وجود دارد. یک جمله با $ [MASK1] $ پوشانده شده و به عنوان متن تولید هدف جملات فاصله‌افتاده استفاده می‌شود. دو جمله دیگر در ورودی باقی می‌مانند و برخی از نشانه‌ها به طور تصادفی توسط$ [MASK2] $پوشانده می‌شوند
		 \cite{zhang2020pegasus}.}
\end{figure}

کدیا
\LTRfootnote{Kedia}
و همکاران الگوریتم حداکثرسازی نقطه-محصول فرایادگیری (ام‌دات)\LTRfootnote{Meta-Learned Dot-Product Maximization (MDot)}
را برای بهبود پگاسوس پیشنهاد دادند. این الگوریتم بر اساس ایده به حداکثر رساندن حاصل‌ضرب نقطه‌ای بین گرادیان‌های مدل در نقاط مختلف آموزش با استفاده از تکنیکی به نام تفاوت‌های محدود\LTRfootnote{finite differences}
است. این الگوریتم از نظر محاسباتی کارآمد است و می‌تواند برای مدل‌های بزرگ مانند برت اعمال شود و سربار محاسباتی را کاهش بدهد \cite{sherborne2023meta}.
عملکرد مناسب مدل پگاسوس در خلاصه‌سازی متون باعث شده بهترین مدل خلاصه‌سازی متون کوتاه مبتنی بر مدل پگاسوس و تکنیک تنظیم\LTRfootnote{regularization}
ام‌دات باشد.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=6cm]{Mdot.png}
	\end{center}
	\caption{ الگوریتم ام‌دات \cite{sherborne2023meta}}
	\label{fig:Mdot}
	\medskip
	\small{
		محاسبه گرادیان برای به حداکثر رساندن محصول نقطه‌ای با استفاده از تقریب تفاضل محدود و استفاده از آن برای تنظیم گرادیان استاندارد \cite{sherborne2023meta}.
	}
\end{figure}


%\section{روش‌های مبتنی بر مدل‌های از پیش آموزش دیده}



\subsection{ایده‌های ارائه شده بهبود خلاصه‌سازی متون طولانی }


یکی از مشکلات مدل ترنسفومر در خلاصه‌سازی متون طولانی، حافظه‌ی درجه دوم، پیچیدگی‌های محاسباتی و تعداد زیاد عملیات می‌باشد. برای حل این چالش‌ها ایده‌های مختلفی ارائه شده است. به عنوان مثال شبکه‌ی ریفورمر\LTRfootnote{‫‪Reformer‬‬}
برای حل چالش‌های محاسباتی مرتبط با پردازش دنباله‌های طولانی متن ارائه شده است. لایه‌های برگشت‌پذیر\LTRfootnote{reversible layers}
معرفی شده در این مقاله امکان بازسازی ورودی از خروجی را در طول گذر به عقب را فراهم می‌کنند که موجب  کاهش نیازهای حافظه و امکان پردازش کارآمد دنباله‌های طولانی  می‌شود.
علاوه بر این، ریفورمر از تکه تکه کردن برای پردازش بخش‌های کوچک‌تر ورودی به طور مستقل استفاده می‌کند که موازی‌سازی را ممکن می‌کند و مصرف حافظه را کاهش می‌دهد. همچنین استفاده از درهم‌سازی حساس به مکان\LTRfootnote{‫‪locality-sensitive‬‬ ‫‪hashing‬‬ ‪(LSH‬)} 
 در مکانیسم توجه منجر به محاسبه توجه کارآمدتر می‌شود. درهم‌سازی حساس به مکان با توجه به زیرمجموعه‌ای از نشانه‌ها بر اساس مقادیر هش آنها، محاسبه توجه کامل را تقریب می‌زند. علاوه بر این، ریفورمر از کدگذاری‌های موقعیت محوری برای کدگذاری اطلاعات موقعیت توکن‌ها به صورت فشرده استفاده می‌کند. این تکنیک‌ها مجموعاً مدل ریفورمر را  مقیاس‌پذیر می‌سازد، و آن را قادر می‌سازد تا دنباله‌های طولانی متن را مدیریت کند و در عین حال عملکرد رقابتی را در وظایف مختلف پردازش زبان طبیعی حفظ کند \cite{reformer}.
%\todo{ادیت}

 شبکه‌ی ترنسفورمر پراکنده\LTRfootnote{sparse }
با معرفی فاکتورسازی‌ ماتریس پراکنده‌ی توجه، زمان و حافظه مورد نیاز را به کاهش می‌دهد.
با استفاده از پراکندگی، مدل می‌تواند تنها به زیرمجموعه‌ای از نشانه‌های ورودی توجه کند و روی مرتبط‌ترین اطلاعات تمرکز کند و بقیه را نادیده بگیرد. این رویکرد پیچیدگی محاسباتی را کاهش می‌دهد و مدل می‌تواند توالی‌های طولانی‌ را مدیریت کند \cite{child2019generating}.
 مشابه شبکه‌ی ترنسفورمر پراکنده مدل بیگ‌برد که\LTRfootnote{‫‪Big‬‬ ‫‪Bird‬‬}
با استفاده از مکانیزم توجه پراکنده\LTRfootnote{‫‪Sparse‬‬ ‫‪attention‬‬}
 عملکرد ترنسفورمر را در مواجه با دنباله‌ی کلمات
%\LTRfootnote{‫‪sequence‬‬}
طولانی بهبود می‌بخشد، نوآوری‌های دیگری مانند توجه جامع\LTRfootnote{global attention}
را معرفی می‌کند. در این مدل توکن‌های خاص به تمام توکن‌های دیگر در دنباله توجه می‌کنند و وابستگی‌های دوربرد را بهتر از سایر روش‌ها به دست می‌آورند. همچنین  فرآیند پالایش تکراری  وزن‌های توجه را برای بهبود عملکرد مدل اصلاح می‌کند \cite{zaheer2020big}. 
%\todo{ادیت}


%ژیائو و کارنی \LTRfootnote{‫Xiao \& ‬‬ ‫‪Carenini‬‬} با تمرکز برکاهش تکرار و افزونگی در خلاصه‌سازی اسناد طولانی مدلی طراحی کرده‌اند که کدگشایی آن براساس مکانیزم توجه آگاه از افزونگی می‌باشد. علاوه بر این تابع ضرر \LTRfootnote{‫‪Loss‬‬ ‫‪function‬‬} طراحی کرده‌اند که برای تعادل بین اهمیت و افزونگی مناسب باشد. این روش باعث انتخاب جملات مهم و غیر زائد می‌شود \todo{دوباره بنویس} \cite{xiao2020systematically}

در سال‌های اخیر ایده‌های مختلفی برای بهبود کیفیت خروجی مدل خلاصه‌سازی خودکار اسناد بلند ارائه شده است. به عنوان مثال پایل\LTRfootnote{‫‪Pilault‬‬}
و همکاران که برای بهبود خلاصه انتزاعی نهایی متون طولانی از رویکرد ترکیبی استخراجی-انتزاعی با استفاده از مدل زبانی از پیش آموزش دیده جی‌پی‌تی-دو\LTRfootnote{GPT-2}
استفاده می‌کنند. در این مدل مرحله استخراج ساده قبل از تولید خلاصه انجام می‌شود، سپس برای شرطی کردن مدل زبانی ترنسفورمر بر روی اطلاعات مربوط قبل از تولید خلاصه استفاده می‌شود. این رویکرد در مقایسه با کارهای قبلی که از مکانیزم کپی استفاده می‌کنند، خلاصه‌های انتزاعی بیشتری تولید می‌کند \cite{pilault2020extractive}. 
پانگ\LTRfootnote{Pang}
و همکاران یک ساختار سلسله مراتبی برای اسناد طولانی فرض کرده‌اند. در این ساختار سطح بالا بر وابستگی دوربرد تمرکز می‌کند و سطح پایین جزئیات را حفظ می‌کند. 
در استنتاج از پایین به بالا، تعبیه‌های متنی نشانه‌ها با استفاده از توجه محلی محاسبه می‌شوند و برای دریافت وابستگی‌های دوربرد و زمینه جامع، استنتاج از بالا به پایین برای نمایش‌های توکن اعمال می‌شود. یک ساختار پنهان چند مقیاسی دو سطحی استفاده می‌شود، که در آن سطح پایین شامل نمایش‌های نشانه‌ای است که توسط استنتاج پایین به بالا محاسبه می‌شود، سپس با اعمال مکانیزم توجه به سطوح بزرگ‌تر روابط بین بخش‌های مختلف سند را بدست می‌آورد. ساختار مدل را در شکل \ref{fig:top_down} نشان داده شده است. روش پیشنهادی یک رویکرد جدید امیدوارکننده برای خلاصه‌سازی اسناد طولانی است و نسبت به روش‌های قبلی کارآمدتر و موثرتر است \cite{pang2023long}.
% این چارچوب استنتاج از پایین به بالا را با استنتاج از بالا به پایین برای بهبود استنتاج نمایش توکن هم‌افزایی می‌کند 

\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=8cm]{top_down.png}
	\end{center}
	\caption{ معماری مدل ترنسفورمر از بالا به پایین \cite{pang2023long}}
	\label{fig:top_down}
	\medskip
	
\end{figure}


%. روش پیشنهادی یک رویکرد جدید امیدوارکننده برای خلاصه‌سازی اسناد طولانی است و بهترین عملکرد در خلاصه‌سازی متون طولانی را دارد
%\todo{ادیت}

جیدیوتیس و همکاران شیوه‌ی تقسیم و غلبه ( دنسر)\LTRfootnote{Divide-and-ConquER (DANCER)}
را برای بهبود خلاصه‌سازی اسناد طولانی پیشنهاد کرده اند.این روش به طور خودکار خلاصه یک سند را به چند بخش‌ تقسیم می‌کند و هر یک از این بخش‌ها را به بخش مناسب سند جفت می‌کند تا خلاصه‌های هدف متمایز ایجاد کند. شیوه‌ی معرفی شده در نظر می‌گیرد که متون طولانی به صورت بخش‌های گسسته ساختاربندی شده‌اند. 

برای مطابقت هر قسمت از خلاصه با بخشی از سند در دنسر از معیار روژ\LTRfootnote{ROUGE}
استفاده ‌می‌شود. در این روش معیار روژ-ال بین هر یک از جملات خلاصه و تمام جملات سند محاسبه می‌شود و هر جمله‌ی خلاصه هدف به بخش حاوی جمله با بیشترین روژ- ال نسبت داده می‌شود. 
سپس تمام جملات خلاصه‌ی هدف مربوط به هر بخش را به هم الحاق می‌کنیم تا خلاصه‌ی هدف برای هر بخش ایجاد شود. در طول آموزش هر بخش از سند به همراه جمله‌ی خلاصه‌ی مربوط به آن به عنوان متن ورودی و خلاصه‌ی هدف استفاده می‌شود. 
مزایای این روش آموزش:
\begin{enumerate}
	\item {
		تقسیم مساله به چند زیر مساله باعث کاهش پیچیدگی و ساده‌سازی مساله می‌شود.
	}
	\item {
		انتخاب خلاصه‌های هدف برای هر بخش بر اساس امتیازات روژ-ال هر جمله باعث تطابق بهتر و متمرکزتر بین دنباله‌های منبع و هدف ایجاد می‌شود.
	}
	\item {
		تقسیم هر سند آموزشی به چند جفت ورودی-هدف، نمونه‌های آموزشی بسیار بیشتری ایجاد می‌کند. این کار برای مدل‌های خلاصه‌سازی عصبی مفید است. 	
	}
	\item {
		این روش می‌تواند از مدل‌های خلاصه‌سازی مختلف از جمله شبکه‌ی عصبی بازگشتی و ترنسفورمرها استفاده کند.
	}
\end{enumerate}


هنگام کار با اسناد ساختاریافته طولانی، معمولاً همه بخش‌های سند کلیدی برای سند نیستند. اگر یک مقاله آکادمیک را به عنوان مثال در نظر بگیریم، بخش‌هایی مانند مرور ادبیات یا پیشینه در تلاش برای خلاصه کردن نکات اصلی مقاله ضروری نیستند و باعث افزودن نویز می‌شوند. بنابراین از بخش مرور ادبیات صرف نظر می‌شود و تمرکز سیستم خلاصه‌سازی فقط روی بخش‌های مقدمه، روش‌ها، نتایج و نتیجه‌گیری می‌باشد.
%\todo{میتونه حذف بشه }

این مدل قابل ترکیب با پگاسوس یا مدل مولد نقطه‌ای\LTRfootnote{Pointer-Generator model}
می‌باشد.
بخش کدگشا مدل مولد نقطه‌ای با ایجاد جملات تکراری مقابله ‌می‌کند.
هرچند ممکن است به خاطر تکرار اطلاعات در بخش‌های مختلف بازهم خلاصه‌ی تکراری ایجاد شود.

شیونگ و همکاران با اصلاح هدف بهینه‌سازی، معماری مدل‌‌های از پیش آموزش دیده و مجموعه‌ی دادگان پیش‌آموزش\LTRfootnote{pretraining corpus}
روشی را برای ساخت مدل‌های مناسب متون طولانی پیشنهاد می‌کنند. مدل‌های پيش‌آموزش دیده متن به متن، مانند برت و بارت، معمولاً بر روی دنباله‌های متن کوتاه، مانند جملات یا پاراگراف‌ها آموزش داده می‌شوند. در حالی که بسیاری از وظایف پردازش زبان طبیعی، مانند پاسخگویی به سؤال و خلاصه کردن، به توانایی پردازش توالی متن طولانی نیاز دارند این مقاله تعدادی از تکنیک‌ها را برای تطبیق مدل‌های متن به متن از پیش آموزش دیده برای دنباله‌های متن طولانی پیشنهاد می‌کند. این تکنیک‌ها عبارتند از:
\begin{itemize}
	\item{
		ارائه‌ی مدل براساس یک ترنسفورمر با 	مکانیزم توجه به خود پراکنده‌ی بلوکی\LTRfootnote{Block-sparse self-attention}
		در قسمت کدگذار است. این مکانیزم امکان استفاده‌ی مجدد از وزن‌های مدل‌های از پیش آموزش دیده را فراهم می‌کند.	
	}
	
	\item {
		مکانیرم توکن سراسری\LTRfootnote{Global-token mechanism}:
		در این مکانیزم یک مجموعه‌ی کوچک از توکن‌های سراسری به کل توالی توجه می‌کنند و امکان تعاملات دوربرد در کدگذار فراهم می‌شود.
	}
	\item{
		هم‌پوشانی بلوک‌های توجه\LTRfootnote{ Overlapping attention windows}
		: توجه لغزشی با همپوشانی یک راه ساده برای معرفی اتصالات دوربرد در مدل‌های توجه محلی است. در این رویکرد، توکن‌های درون هر بلوک به تمام توکن‌های درون خود بلوک و همچنین نیمی‌از توکن‌های بلوک‌های چپ و راست مجاور نزدیک می‌شوند. این نسخه بلوکی از پنجره‌های توجه همپوشانی، راه ساده‌تر و کارآمدتری را برای معرفی اتصالات دوربرد ارائه می‌کند و در عین حال موازی‌سازی را در پیاده‌سازی مدل تسهیل می‌کند.
		
	}
	\item{
		لایه‌ی خود توجه مبتنی بر ادغام بلوکی تقویت شده\LTRfootnote{Pooling-augmented blockwise attention}:
		این لایه به عنوان جایگزین لایه خود توجهی برای اتصالات دوربرد معرفی شده است. این رویکرد به واحدهای توجه درون بلوک‌ها اجازه می‌دهد تا به جای توجه به همسایگان بلافصل خود، بر خلاصه‌ای از اطلاعات کلی در بلوک‌ها تمرکز کنند. این لایه در تصویر \ref{fig:attention_pooling}نشان داده شده است. این مدل را قادر می‌سازد تا از اطلاعات گسترده تری در سراسر سند برای تصمیم گیری استفاده کند و وابستگی‌های دوربرد را در نظر بگیرد. با بکارگیری عملیات ادغام، ابعاد و نمایش بردارهای توجه کاهش می‌یابد که منجر به افزایش سرعت و کارایی مدل می‌شود.}
\end{itemize}
نویسندگان تکنیک‌های پیشنهادی را در تعدادی از وظایف توالی متن طولانی، از جمله پاسخ‌گویی به سؤال و خلاصه‌نویسی، ارزیابی کرده‌اند. نتایج نشان می‌دهد که مدل‌های اقتباس‌شده در تمامی‌وظایف از مدل‌های پایه بهتر عمل می‌کنند.
این تکنیک‌ها استفاده از مدل‌های متنی از پیش آموزش دیده را برای طیف وسیعی از وظایف پردازش زبان طبیعی ممکن می‌سازد
 \cite{Xiong2022AdaptingPT}.
\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=8cm]{pooling_attention.png}
	\end{center}
	\caption{ لایه خودتوجهی تقویت شده ادغام شده \cite{Xiong2022AdaptingPT}}
	\label{fig:attention_pooling}
	\medskip
	\small
\end{figure}
کارهای تحقیقاتی در زمینه ی یادگیری تقویتی\LTRfootnote{reinforcement learning}
و پردازش زبان طبیعی در سال‌های اخیر رشد کرده است. در یادگیری تقویتی  یک عامل با محیط تعامل می‌کند و با آزمون و خطا، خط مشی بهینه را برای تصمیم گیری متوالی برای به حداکثر رساندن پاداش تجمعی آینده می‌آموزد. این پاداش می‌تواند یک معیار تعریف شده توسط توسعه دهنده بر اساس کار در حال حل باشد. در خلاصه‌سازی خودکار انتزاعی متن، نمونه‌هایی از چنین پاداش‌هایی ممکن است شامل حفظ برجستگی، مستلزم منطقی هدایت‌شده، و غیر افزونگی باشد. به طور کلی،  یادگیری تقویتی  در چهار حوزه مختلف برای بهبود خلاصه‌سازی خودکار استفاده می‌شود:

\section{روش‌های مبتنی بر یادگیری تقویتی}
\subsection{ یادگیری تقویتی برای حل چالش‌های مدل دنباله به دنباله عمیق}

استفاده از یادگیری تقویتی به منظور حل مسائل گوناگونی که مدل‌های دنباله به دنباله عمیق قادر به حل آن‌ها نیستند، امکانات بیشتری را فراهم می‌کند. به عنوان مثال، مشکلاتی مانند کمبود نوآوری در ایجاد خلاصه‌های خلاقانه و آموزنده و کاهش کیفیت خلاصه‌ها در صورت افزایش طول مقالات منبع، با استفاده از سیستم‌های یادگیری تقویتی و یادگیری خط‌مشی\LTRfootnote{ policy learning} بهبود یافته است.
علاوه بر این  مدل‌های دنباله به دنباله عمیق را نمی‌توان برای خلاصه کردن طیف گسترده ای از اسناد استفاده کرد، زیرا مدلی که بر روی یک مجموعه داده آموزش داده می‌شود، در یک مجموعه داده دیگر به خوبی عمل نمی‌کند و قابلیت تعمیم ندارد. رویکردهای مبتنی بر یادگیری تقویتی می‌تواند این  مشکل را با استفاده ازگرادیان خط مشی انتقادی \LTRfootnote{self-critic policy gradient}
و ترکیب آن با یادگیری انتقالی\LTRfootnote{Transfer Learning (TL) }
برای انتقال دانش از یک مجموعه داده به مجموعه دیگر برطرف کنند\cite{DeepTL_RL}.

فریم‌ورک پوبرل\LTRfootnote{PoBRL}
(ترکیب سیاست‌ها با حداکثر ارتباط حاشیه‌ای و یادگیری تقویتی) اهمیت، ارتباط و طول خلاصه را در زمینه خلاصه‌سازی چند سندی با جدا کردن مسئله بهینه‌سازی چند هدفه به مسائل فرعی کوچک‌تر که با استفاده از یادگیری تقویتی قابل حل هستند، بهینه می‌کند. 
اهمیت، ارتباط و طول خلاصه را در زمینه خلاصه‌سازی چند سندی با جدا کردن مسئله بهینه‌سازی چند هدفه به مسائل فرعی کوچک‌تر که قابل حل هستند، بهینه می‌کند.
این فریم‌ورک از الگوریتم حداکثر ارتباط حاشیه ای\LTRfootnote{ Maximal Marginal Relevance (MMR)}
برای استخراج اطلاعات مهم از اسناد استفاده می‌کند. استفاده از این الگوریتم باعث افزایش ارتباط بین  جملات و کاهش افزونگی می‌شود.
در ادامه با از یادگیری تقویتی رای بهینه سازی هر هدف به صورت جداگانه استفاده می کند و خط مشی های جداگانه ای را برای اهمیت، ارتباط و طول می آموزد\cite{PoBRL}.
خلاصه‌سازی چند سندی شامل سر و کار داشتن با اطلاعات پیچیده و همپوشانی از منابع متعدد است. الگوریتم‌های یادگیری تقویتی می‌توانند با مدل‌سازی خلاصه‌سازی به عنوان یک فرآیند تصمیم‌گیری متوالی  پیچیدگی را مدیریت کنند و یاد بگیرند جملات مرتبط حاوی اطلاعات را برای خلاصه انتخاب کنند. علاوه بر این  یادگیری تقویتی امکان بهینه‌سازی همزمان اهداف متعدد مانند اهمیت، افزونگی و طول را فراهم می‌کند و موجب برقراری تعادل بین اهداف و تولید خلاصه‌‌های مختصر، مرتبط و غیر تکراری شوند.

%در مرحله بعد، چارچوب از یادگیری تقویتی برای بهینه سازی هر هدف به صورت جداگانه استفاده می کند. خط مشی های جداگانه ای را برای اهمیت، ارتباط و طول می آموزد.
%چارچوب مبتنی بر یادگیری تقویتی برای خلاصه سازی چندسندی است که برای بهبود ارتباط بین جملات  و گنجاندن مطالب با اهمیت در خلاصه‌ی خروجی ارائه شده است.این چهارجوب با
سلیکیلماز\LTRfootnote{Celikyilmaz}
و همکاران مدل کدگذار-کدگشای چندعامله را برای بهبود خلاصه‌سازی اسناد طولانی با استفاده از عامل تعامل‌کننده\LTRfootnote{communicating agent}
ارائه کرده‌اند. این مدل وظیفه کدگذاری یک متن طولانی را بین چندین عامل همکاری تقسیم می‌کند، که هر کدام مسئول یک زیربخش از ورودی هستند. این عوامل برای به اشتراک گذاشتن اطلاعات پایه‌ی جامع و ایجاد یک خلاصه متمرکز و منسجم با یکدیگر ارتباط برقرار می‌کنند.  مدل ارائه شده در مقایسه با سایر مدل‌ها عملکرد بهتری دارد و خلاصه‌سازی اسناد طولانی با مدل‌های دنباله به دنباله را بهبود می‌بخشد.
\subsection{یادگیری تقویتی برای ترکیب خلاصه‌های استخراجی و انتزاعی} 
از یادگیری تقویتی برای ترکیب ویژگی‌های استخراجی با خلاصه انتزاعی برای استفاده از هر دو نوع خلاصه ی خودکار با الهام از رفتار انسان استفاده می‌شود. این مدل‌ها ابتدا برجسته‌ترین جملات را از سند ورودی استخراج می‌کنند، سپس با استفاده از دو شبکه: شبکه‌های استخراج‌کننده و انتزاعی، آنها را انتزاع می‌کنند. به عنوان مثال لیو\LTRfootnote{Liu}
و همکاران یک چارچوب متخاصم را پیشنهاد می‌کنند که مدل‌های انتزاعی و استخراجی را همزمان با استفاده از گرادیان خط ‌مشی برای بهینه‌سازی مدل انتزاعی برای خلاصه‌ای با پاداش بالا، آموزش می‌دهد که منجر به خلاصه‌ای منسجم‌تر می‌شود\cite{liu2018generative}.
همچنین چن و بانسال\LTRfootnote{Chen and Bansal}
یک مدل خلاصه‌سازی سریع پیشنهاد کردند که جملات برجسته را استخراج می‌کرد و سپس با استفاده از گرادیان خط مشی سطح جمله مبتنی بر یادگیری‌تقویتی بازنویسی می‌کرد\cite{chen2018fast}.
کریسینسکی \LTRfootnote{Kryscinski}
و همکاران  دو روش برای افزایش سطح انتزاع در خلاصه سازی  پیشنهاد می‌کنند: تجزیه رمزگشا به یک شبکه متنی و یک مدل زبانی از پیش آموزش‌دیده، و بهبود معیار جدید از طریق یادگیری خط‌مشی.
تکنیک اول شامل یک شبکه‌ی محتوایی\LTRfootnote{contextual network}
و یک مدل زبانی از پیش آموزش دیده است. شبکه‌ی محتوایی  بخش‌های مرتبط از سند منبع را بازیابی کرده و آنها را فشرده می‌کند. مدل زبان از پیش آموزش حاوی دانش قبلی در مورد تولید زبان است. این تفکیک مسئولیت ها امکان استخراج بهتر و تولید جملات مختصر را فراهم می کند.
تکنیک دوم شامل معرفی یک معیار جدید است که از طریق یادگیری خط مشی بهینه می‌شود. این معیار مدل را به تولید عبارات بدیع که در سند   منبع وجود نداشته‌اند تشویق می‌کند. با ترکیب این معیار جدید با معیار روژ که همپوشانی کلمات را با خلاصه حقیقت پایه اندازه گیری می‌کند، مدل قادر به تولید خلاصه‌های انتزاعی با عملکرد بالا در همپوشانی کلمات می‌شود \cite{kryscinski-etal-2018-improving}.

\subsection{یادگیری تقویتی  برای ایجاد معیارها و پاداش‌های جدید}

خلاصه‌سازی اسناد، مانند سایر کارهای مولد زبان،  اغلب به دلیل استفاده از اهداف آموزشی مبتنی بر  درست‌نمایی بیشینه\LTRfootnote{maximum likelihood}
مورد انتقاد قرار گرفته است.
درست‌نمایی بیشینه کیفیت خلاصه‌ی تولید شده را در نظر نمی‌گیرد و ممکن است خلاصه‌هایی تولید کند که فقط یک کپی از اسناد ورودی هستند یا پر از کلمات بی‌معنی هستند. به همین دلیل، یادگیری تقویتی به عنوان جایگزینی برای بهینه‌سازی مستقیم مدل‌ها بر روی معیارهای ارزیابی و پاداش صریح به کیفیت پیش‌بینی‌های مدل استفاده شده است	\cite{Parnell2022AMC}. 
معیارهای ارزیابی خلاصه‌سازی مانند روژـ۱\LTRfootnote{ROUGE-1}
،روژ-۲\LTRfootnote{ROUGE-2}،
روژ-‌ال\LTRfootnote{ROUGE-L}
و امتیازبرت\LTRfootnote{BERTScore}
به عنوان پاداش در رویکردهای یادگیری تقویتی استفاده شده است. با این حال، پارنل و همکاران  استدلال می‌کند که استفاده از امتیازات روژ به عنوان پاداش، جنبه‌های مهم خلاصه‌سازی، مانند خوانایی، روان بودن و اشتراک اطلاعات بین اسنادی در خلاصه‌سازی چند سندی را نادیده می‌گیرد و یک پاداش پوشش اصلاح شده همراه با یک برآوردگر گرادیان سیاست مبتنی بر اصول (ریلکس )	 \LTRfootnote{modified coverage reward along with a principled policy gradient estimator (RELAX)}
را پیشنهاد می‌دهند\cite{Parnell2022AMC, ALOMARI}.
ریلکس یک برآوردگر گرادیان خط مشی\LTRfootnote{policy}
با واریانس کم و بدون سوگیری\LTRfootnote{bias}
است که برای مسائل یادگیری تقویتی با فضاهای کنش مداوم، مانند خلاصه‌سازی متن، مناسب است\cite{Grathwohl2017BackpropagationTT}.

در عبارت\ref {eq:relax} تابع زیان ارائه شده برحسب ریلکس نمایش داده شده است.
بخش اول این عبارت سیاست را به تولید خروجی‌هایی با پاداش مورد انتظار بالا و بخش دوم به تولید خروجی‌‌های مشابه خروجی‌های قبلی 
تشویق می‌کند.
%	  تشویق می‌کند تا خروجی‌هایی تولید کند که پاداش مورد انتظار بالایی دارند و بخش دوم سیاست را تشویق می‌کند تا خروجی‌هایی مشابه خروجی‌های تولید شده در گذشته تولید کند همچنین
در این عبارت
$ r $
نشان دهنده‌ی پاداش 
$  c_\phi(\tilde{z}) $
یک متغیر کنترلی از پارامترهای $ φ $ است که انتظار می‌رود با پاداش کاهش واریانس همبستگی داشته باشد.
$  p(yـs) $ 
احتمال دنباله مشاهده شده خروجی $ y_s $ است.
$ z $
دنباله نمونه‌های $Gumbel-Softmax  $ است.
$ \tilde{z} $
دنباله ای از نمونه‌ها از یک توزیع $ Gumbel-Softmax $ مشروط بر $ y_s $ است.


\begin{equation}
	\label{eq:relax}
	L_\text{$ RELAX $} = -[r - c_\phi(\tilde{z})]   \log p(y^{s}) \quad + c_\phi(z) - c_\phi(\tilde{z})
\end{equation}
%	  \todo{یک بار دیگه متغیرها رو چک کن}


\subsection{ یادگیری تقویتی برای ایجاد خلاصه متناسب با نیاز کاربر}
درخلاصه‌سازی متن، یادگیری تقویتی می‌تواند نقش مهمی به عنوان یک رویکرد پیشرفته برای ارائه خلاصه‌های متناسب با نیاز کاربر ایفا کند. با استفاده از یادگیری تقویتی، سیستم‌ها قادر به تحلیل و فهم متن‌ها و درک نیازهای کاربران می‌شوند، سپس با اعمال تصمیمات متناسب، خلاصه‌هایی ایجاد می‌کنند که بیان کننده اصلی‌ترین اطلاعات و مفاهیم موجود در متن اصلی هستند. این رویکرد توانایی ارائه خلاصه‌های متناسب با نیازهای کاربر را بهبود می‌بخشد و تجربه خواندن و درک محتوای متن را بهبود می‌بخشد. همچنین، با استفاده از یادگیری تقویتی، سیستم‌ها قادر به بهبود خودکار خلاصه‌سازی و افزایش کیفیت خلاصه‌های تولید شده هستند. %\todo{تغیر کنه}
%\todo{مقدمه میخواد}
سایر روش‌های خلاصه‌سازی به کاربران اجازه نمی‌دهند، سلیقه‌ی خود را برای کنترل  جنبه‌های مختلف خلاصه‌های تولیدشده نشان بدهند. 

مدل کنترل‌سام
\LTRfootnote{controlSum}
با  افزودن توکن‌های کنترلی به ابتدای متن ورودی و استفاده از یک مدل کدگذار-کدگشا به کاربران اجازه‌ی اعمال ویژگی‌های مورد نیازهای خود بر خلاصه را می‌دهند. به عنوان مثال برای کنترل طول خلاصه خروجی ده طول مجزا تعریف می‌شود و هریک توکن‌های کنترلی نشانگر  یکی از این طول‌ها هستند. هدف آموزش این مدل از طریق تابع زیان درست‌نمایی بیشینه \LTRfootnote{maximum liklihood loss}
است  \cite{fan-etal-2018-controllable}. این هدف آموزش هیچ سیگنال نظارتی صریحی ندارد.
برای حل این مشکل چان  \LTRfootnote{Chan}
و همکاران
با اعمال محدودیت بر روی هدف آموزشی با استفاده از فرآیند تصمیم‌گیری مارکوف محدود  \LTRfootnote{Constrained Markov Decision Process (CMDP)}
یک چهارچوب خلاصه سازی پیشنهاد کرده‌اند که شامل یک تابع پاداش همراه با مجموعه‌ای از  محدودیت ها است و کنترل خلاصه سازی را تسهیل می‌کند. 
هدف عامل بیشینه کردن  پاداش مورد انتظار در عین اعمال محدودیت بر هزینه‌ها است. 
با داشتن این هدف، تصمیم‌گیرنده سعی می‌کند سیاستی را انتخاب کند که منجر به بیشینه کردن پاداش کلی تجمعی در طول زمان شود، در حالی که محدودیت‌ها بر هزینه‌ها رعایت شوند.
این هدف مدل را تشویق می‌کند که خلاصه‌ای شبیه خلاصه‌ی تولید شده توسط انسان تولید کند. با استفاده از این مدل کاربران می‌توانند طول ، مبزان فشردگی و محتوای خلاصه را کنترل کنند به عنوان مثال توضیحات یک محصول را به گونه‌ای خلاصه کند که در یک محدودیت کلمه در تبلیغات آنلاین قرار گیرد.
برای تبدیل مسئله محدود به مسئله بدون محدودیت   از ساده‌سازی لاگرانژ \LTRfootnote{Lagrangian relaxation}
و برای بهینه سازی از الگوریتم بهینه سازی مبتنی بر گرادیان، مانند ادام استفاده می‌شود.
برای اندازه‌گیری شباهت بین خلاصه خروجی و  مرجع بر اساس تعبیه‌های متنی برت
به عنوان تابع پاداش از امتیازبرت‌ استفاده می‌شود. 
برای کنترل تمرکز خلاصه بر روی یک موجودیت نامدار  \LTRfootnote{named entity}
ابتدا ارجاع موجودیت نامدار به سند اضافه می‌شود سپس یک محدودیت سوال و جواب اعمال می‌شود. 
این محدودیت بر روی امتیاز اف‌‌-۱ خروجی یک مدل سوال جواب که ورودی آن شامل یک سوال راجع به موجودیت نامدار و خلاصه‌ی تولید شده است اعمال می‌شود. 
علاوه بر این دو محدودیت عدم تکرار ترای‌گرم  \LTRfootnote{trigram}
و موجودیت‌های درخواستی برای افزایش خوانایی و کاهش تکرار در متن اعمال می‌شود.
%این محدودیت شامل یک سوال راجع به موجودیت نامدار و خلاصه‌ی تولید شده است که به عنوان ورودی به یک مدل سوال و جواب داده می‌شود و امتیاز اف‌‌-۱ ‌برای خروجی مدل محاسبه می‌شود.
مدل اینت‌سام    \LTRfootnote{IntSumm}
یک مدل خلاصه‌سازی تعاملی با هدف خلاصه کردن اطلاعات مهم بر اساس کوئری‌های \LTRfootnote{query}
کاربر و ارائه کوئری پیشنهادی برای کمک به کاربران است. در ابتدا این مدل یک خلاصه‌ی اولیه تولید می‌‌کند و به کاربر نمایش می‌دهد سپس یک کوئری از کاربر دریافت می‌کند و خلاصه‌ی اولیه به همراه پاسخ کوئری را به کاربر نمایش می‌دهد.
برای ارزیابی مدل ارائه شده مساحت منحنی بازیابی  \LTRfootnote{recall}
بر اساس طول خلاصه معرفی شده است که ستون عمودی آن امتیاز بازیابی روژ و ستون افقی آن طول خلاصه مرجع می‌باشد و مساحت بیشتر زیر منحنی نشان دهنده‌ی مدل بهتر است. یک نمونه از این نمودار در شکل \ref{fig:recall_curve}   نمایش داده شده است
\cite{shapira-etal-2021-extending}.


\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=8cm]{recall cureve.png}
	\end{center}
	\caption{ یک نمونه از نمودار منحنی بازیابی بر اساس طول \cite{shapira-etal-2021-extending}}
	\label{fig:recall_curve}
	\small{این نمودار دو تعامل متفاوت با سیستم خلاصه‌سازی را مقایسه می‌کند. هر نقطه نمایانگر   خروجی هر مرحله تعامل با کاربر است.}
	\medskip
	
\end{figure}




شاپیرا و همکاران برای بهبود مدل اینت‌سام و بهبود سرعت عمل در پاسخ‌گویی، توانایی پردازش کامل متون طولانی و رعایت تعادل میان اطلاعات کلی مقاله و اطلاعات مورد نیاز کاربر یک مدل جدید ارائه داده‌‌اند.
ورودی این مدل  مجموعه‌ی اسناد، کوئری و تاریخچه‌ی تعاملات با کاربر به همراه خروجی‌ قبلی است.
در ابتدا تعبیه کوئری به تعبیه اسناد ورودی الحاق شده  سپس امتیاز $ qMMR $ با استفاده از مدل $ RL-MMR $ ‌محاسبه می‌شود. هدف این امتیاز  ایجاد خلاصه‌ای شبیه به اسناد ورودی و کوئری و متفاوت از تاریخچه است.
سپس با استفاده از مکانیزم توجه با مرکزیت دوگانه  \LTRfootnote{two hub attention}
بر اساس کدگذاری به دست آمده از تاریخچه و مدل $ RL-MMR $ ‌

توزیع احتمال هر جمله را به دست ‌می‌آید.
مدل ام‌سام\LTRfootnote{MSumm}
یک مدل خودرگرسیون\LTRfootnote{Autoregressive}
است  که برای آموزش آن از یادگیری تقویتی به همراه مکانیسم پاداش دوگانه استفاده می‌شود.   معیار دلتا-روژ\LTRfootnote{Delta-ROUGE}
برای سنجش میزان اطلاعات اضافه‌ی خروجی نسبت به خروجی‌های قبلی و شباهت واژگانی و معنایی  برای سنجش میزان شباهت خروجی به کوئری به عنوان پاداش استفاده شده‌اند.
مدل $ RL-MMR $ ‌ موجب افزایش سرعت پردازش اطلاعات در مدل و پردازش کامل مجموعه‌ی اسناد و مکانیزیم پاداش دوگانه  تعادل موجب ایجاد تعادل اطلاعات می‌شود.    ساختار مدل در شکل  \ref{fig:MSumm}  نشان داده شده است
\cite{shapira-etal-2022-interactive}.
% برای پردازش سریع و کامل متون از مدل $ RL-MMR $ ‌و برای رعایت تعادل از یک مکانیسم پاداش دوگانه  (میزان شباهت خروجی به کوئری و میزان اطلاعات اضافه‌ی خروجی نسبت به خروجی‌های قبلی )استفاده شده است. 


\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=7cm]{query-assited.png}
	\end{center}
	\caption{ معماری مدل  ام‌سام \cite{shapira-etal-2022-interactive}} 
	\label{fig:MSumm}
	
	\medskip
	
\end{figure}


%  ایجاد معیارهای جدید ارزیابی براساس منابعی غیر از خلاصه‌های مبنایی2. معیار روژ   دارای سه محدودیت اصلی است: تعصب آن نسبت به شباهت واژگانی، توجه کم آن به روان بودن و خوانایی خلاصه‌های انتزاعی تولید شده ، و پیش نیاز سخت آن برای استفاده از خلاصه‌های حقیقت پایه برای تولید امتیازات. علاوه بر این، خلاصه‌های تولید شده با معیار روژ بالا معمولاً جذابیت انسانی پایینی دارند ، بنابراین، محققان معیارهای جدیدی را برای افزایش تازگی [16]، سازگاری واقعی[17] و کیفیت بر اساس پاسخ به پرسش و رتبه‌بندی انسانی [17] با استفاده از رویکردهای پاداش‌دهی یادگیری تقویتی بدون نیاز به خلاصه‌های مبنایی ایجاد کردند.




%استفاده از برآورد درست‌نمایی بیشینه  در مدل‌های خلاصه‌سازی  ممکن است  با. به همین دلیل، یادگیری تقویتی به عنوان جایگزینی برای بهینه‌سازی مستقیم مدل‌ها بر روی معیارهای ارزیابی و پاداش صریح به کیفیت پیش‌بینی‌های مدل استفاده شده است. 


%مدل ‌$ awsome  $ با استفاده از 
برای متون طولانی یک روش بهینه ارائه میدهد.
مدل $ AWESOME $ از یک روش جدید دو مرحله‌ای برای بهبود خلاصه سازی متون طولانی استفاده می‌کند: استفاده از حافظه خارجی و شناسایی مفاهیم برجسته در کل سند
\LTRfootnote{global salient content identification}
. حافظه‌های خارجی در طول فرآیند خلاصه سازی قابل دسترسی هستند و بخش‌های کدگذاری شده سند و خلاصه‌های مربوط به آن‌ها را ردیابی می‌کنند تا درک جامع عمیق‌تر و انسجام خلاصه را تقویت کنند. علاوه بر این، محتوای برجسته‌ی جامع از بخش‌های گذشته و آینده استخراج می‌شود تا هر بخش را در حین کدگذاری تقویت کند و اطمینان حاصل شود که موضوعات مهم در خلاصه مورد توجه قرار می‌گیرند. با بهره‌گیری از این مکانیزم‌ها و یک معماری مبتنی بر حافظه کارآمد، این روش در زمینه‌های اطلاعاتی، انسجام و وفاداری نسبت به روش‌های قبلی عملکرد بهتری دارد
\ref{Cao2023AWESOMEGM}.
\section{روش های مبتنی بر مدل‌های زبانی بزرگ و چالش‌ها}
 با استفاده از روش تنظیم پرامپت \LTRfootnote{Prompt Tuning)} توانستند عملکرد مدل‌های زبانی بزرگ را بهبود بخشند. آن‌ها به جای  تنظیم دقیق پارامتر های  مدل‌های فشرده‌شده  که با روش‌هایی مانند کوانتیزشن و هرس مدل  انجام می‌شود، تمرکز خود را بر سازگاری ورودی‌ها قرار دادند. در این روش، پرامپت‌های نرم
  \LTRfootnote{soft prompts} به عنوان توکن‌های یادگیرنده به ابتدای توالی ورودی اضافه شده و از طریق بهینه‌سازی احتمال خروجی مدل تنظیم 
  شدند، بدون آن‌که پارامترهای مدل فشرده تغییر کنند. این اعلان‌ها توانستند افت دقت ناشی از فشرده‌سازی را جبران کرده و حتی عملکرد مدل را در برخی موارد به سطح مدل‌های اصلی برسانند. ویژگی مهم این اعلان‌ها انتقال‌پذیری بالا بود، به‌طوری که در محیط‌ها، روش‌های فشرده‌سازی و وظایف مختلف قابل استفاده بودند. نتایج نشان داد که این روش سبک و کم‌هزینه می‌تواند تعادل میان کارایی و دقت مدل‌های فشرده را به شکلی مؤثر برقرار کند\cite{Xu2024SoftPR}.
\subsection{هزیان‌گویی در مدل ‌های زبانی بزرگ}
 زیوی جی و همکارانش به بررسی مسئله هزیان‌گویی در مدل‌های زبان بزرگ پرداخته‌اند، پدیده‌ای که در آن مدل‌ها اطلاعاتی نامعتبر یا غیرواقعی تولید می‌کنند. این پژوهش با تمرکز بر کاربردهای پزشکی، روشی تعاملی مبتنی بر خودبازتابی ارائه می‌دهد که از طریق یک چرخه تکراری تولید، امتیازدهی و اصلاح، اعتبار پاسخ‌ها را افزایش می‌دهد. آزمایش‌های انجام شده نشان‌دهنده کاهش قابل توجه هزیان‌گویی و افزایش قابلیت اطمینان سیستم‌ها در پاسخگویی به سوالات پزشکی است\cite{ji-etal-2023-towards}​.

. ییجون شیاو و همکارانش رابطه بین هزیان‌گویی و عدم‌قطعیت پیش‌بینی در تولید زبان شرطی را بررسی کرده‌اند. آن‌ها با تاکید بر نقش مهم عدم‌قطعیت اپیستمیک، روشی برای بهبود الگوریتم جستجوی پرتو پیشنهاد می‌دهند که با کاهش عدم‌قطعیت، میزان هزیان‌گوییات تولید شده توسط مدل را کاهش می‌دهد. این پژوهش در وظایفی مانند توصیف تصویر و تولید داده به متن انجام شده و نتایج مثبت قابل توجهی در کاهش محتوای غیرواقعی نشان داده است\cite{xiao-wang-2021-hallucination}.

 دنیل کینگ و همکارانش روشی به نام $PINOCCHIO$ برای بهبود انسجام سیستم‌های خلاصه‌سازی انتزاعی ارائه داده‌اند. این روش با اعمال محدودیت‌هایی در جستجوی پرتو، تولید خروجی‌هایی را که به منبع متن مرتبط نیستند کاهش می‌دهد. نتایج آزمایش‌ها نشان داده که این رویکرد می‌تواند انسجام متون تولید شده را به طور قابل توجهی افزایش داده و هزیان‌گوییات را کاهش دهد، بدون آنکه بر روانی متن تاثیر زیادی بگذارد\cite{king-etal-2022-dont}​.

، جاشوا ماینز و همکارانش به بررسی مسئله هزیان‌گویی در خلاصه‌سازی انتزاعی اسناد پرداخته‌اند. آن‌ها نشان می‌دهند که مدل‌های تولید متن شرطی اغلب محتوایی تولید می‌کنند که با متن منبع سازگار نیست و این پدیده را به عنوان "هزیان‌گویی درونی" و "هزیان‌گویی بیرونی" طبقه‌بندی می‌کنند. این مقاله از طریق ارزیابی انسانی نشان می‌دهد که بیش از 70٪ خلاصه‌ها شامل محتوای هزیان‌گویی‌آمیز هستند که اکثریت آن‌ها به‌ویژه در هزیان‌گویی بیرونی، نادرست می‌باشند. نویسندگان همچنین پیشنهاد می‌کنند که مدل‌های از پیش آموزش‌دیده مانند $BERTS2S$ نسبت به مدل‌های دیگر، خلاصه‌های دقیق‌تر و با هزیان‌گویی کمتر تولید می‌کنند. این پژوهش با ارائه معیارهایی جدید برای ارزیابی دقت و اعتبار خلاصه‌ها، مسیر بهتری برای بهبود ارزیابی‌های خودکار و روش‌های تولید خلاصه‌سازی باز می‌کند​\cite{maynez-etal-2020-faithfulness}.
 جورج کریسوستومو و همکارانش تأثیر هرس مدل‌های زبان بزرگ بر کاهش هزیان‌گویی در خلاصه‌سازی متون را بررسی کرده‌اند. آن‌ها با استفاده از روش‌های پیشرفته هرس، مانند $SparseGPT$ و $Wanda$، نشان داده‌اند که مدل‌های هرس‌شده نسبت به مدل‌های اصلی هزیان‌گویی کمتری دارند. این مدل‌ها بیشتر بر متن منبع تکیه کرده و خلاصه‌هایی با همپوشانی واژگانی بالاتر و محتوای واقع‌گرایانه‌تر تولید می‌کنند. آزمایش‌ها روی پنج مجموعه داده مختلف و چندین مدل، کاهش قابل‌توجه ریسک هزیان‌گویی را با افزایش میزان هرس نشان داده است. این پژوهش، استفاده از مدل‌های هرس‌شده را به‌عنوان راهکاری مؤثر برای کاهش هزیان‌گویی در خلاصه‌سازی پیشنهاد می‌کند.
 
مسئله هزیان‌گویی در مدل‌های زبان بزرگ یکی از چالش‌های اساسی در تولید زبان طبیعی است که می‌تواند اعتبار و اطمینان به این مدل‌ها را تحت تأثیر قرار دهد. استفاده از روش‌های مبتکرانه و دقیق می‌تواند به طور قابل توجهی میزان هزیان‌گویی را کاهش داده و انسجام و دقت خروجی‌های مدل را بهبود بخشد. این پیشرفت‌ها راه را برای استفاده ایمن‌تر و موثرتر از مدل‌های زبان در کاربردهای حساس، از جمله پزشکی و خلاصه‌سازی متون، هموار می‌کند.

\subsection{هرس مدل های زبانی}

در حوزه بهینه‌سازی مدل‌های زبانی بزرگ، روش‌های مختلفی برای کاهش پیچیدگی محاسباتی و منابع مورد نیاز ارائه شده‌اند.
فانگ و همکاران الگوریتمی برای هرس ساختاریافته مدل‌های زبانی بزرگ ارائه داده‌اند که با وابستگی کمتر به داده، نیازی به مجموعه داده کامل ندارد و در مدت کوتاهی قابل اجرا است. الگوریتم $LLM-Pruner$ شامل سه مرحله کلیدی است: ابتدا وابستگی بین اجزای مدل شناسایی می‌شود، سپس اهمیت هر بخش وابسته به‌صورت مستقل از وظایف \LTRfootnote{task-agnostic} ارزیابی می‌شود و در نهایت، با استفاده از روش لورا، عملکرد مدل با حداقل داده بازیابی می‌شود. این روش توانسته با هرس ۲۰ درصد از مدل، ۹۴ درصد از کارایی اولیه را حفظ کند. نتایج این تحقیق نشان می‌دهد که رویکرد پیشنهادی با شناسایی و هرس بخش‌های وابسته مدل به صورت داده‌محور و بدون نیاز به به‌روزرسانی وزن‌ها، روشی کارآمد و سریع برای کاهش پیچیدگی مدل‌های زبانی بزرگ ارائه می‌دهد\cite{men2024shortgpt}.


 ژانگ و همکاران  الگوریتم $D-PRUNER $را معرفی کرده‌اند. این الگوریتم با هدف ارائه یک روش هرس داده‌محور و غیرساختاری طراحی شده که ضمن حفظ دانش عمومی مدل، توانایی آن را در فهم دانش دامنه خاص نیز حفظ کند. رویکرد $D-PRUNER$ در سه مرحله کلیدی شامل شناسایی اهمیت وزن‌های عمومی، بهینه‌سازی تابع ضرر با اضافه کردن  عبارت منظم‌سازی     \LTRfootnote{regularization term} برای جلوگیری از تغییر وزن‌های مهم، و در نهایت هرس وزن‌های کم‌اهمیت با استفاده از "فیشر تجربی" پیاده‌سازی می‌شود. نتایج این تحقیق نشان می‌دهد که این روش نه تنها پیچیدگی محاسباتی مدل را کاهش می‌دهد، بلکه با حفظ توازن میان دانش عمومی و خاص، عملکرد مدل را در دامنه‌های تخصصی بهبود می‌بخشد و نرخ سردرگمی \LTRfootnote{perplexity} کمتری نسبت به روش‌های دیگر ارائه می‌دهد\cite{zhang2024pruning}.

یکی از پژوهش‌های کلیدی در زمینه کاهش پیچیدگی مدل‌های زبانی بزرگ، مقاله‌ای از $Xin Men$ است. این تحقیق بر شناسایی  اضافه‌بودگی   \LTRfootnote{redundancy}در لایه‌های شبکه عصبی مدل‌های زبانی بزرگ تمرکز دارد. یافته اصلی نشان می‌دهد که بسیاری از لایه‌های میانی و انتهایی مدل تغییرات محدودی در حالات پنهان ایجاد می‌کنند و می‌توان آن‌ها را با حداقل تأثیر بر عملکرد مدل حذف کرد.

برای ارزیابی اهمیت هر لایه، متریک $Block Influence (BI)$ معرفی شده است. این متریک میزان تغییر حالات پنهان\LTRfootnote{hidden states} پس از عبور از هر لایه را می‌سنجد. $BI$ بر اساس شباهت کسینوسی بین ورودی و خروجی لایه تعریف شده است؛ به این صورت که $BI$ پایین‌تر نشان‌دهنده تغییرات کمتر و اهمیت کمتر لایه است.

با استفاده از این متریک، لایه‌های با $BI$ پایین شناسایی و حذف می‌شوند. این روش توانسته است با حذف ۲۵٪ از لایه‌های مدل، حدود ٪۹۰ از عملکرد اولیه را حفظ کند و در عین حال، از روش‌های پیشرفته دیگر در این زمینه پیشی بگیرد. همچنین، این روش با تکنیک‌های کوانتایزه‌سازی سازگار است و امکان کاهش بیشتر محاسبات و پارامترها را فراهم می‌کند.

\begin{equation}
	BI_i = 1 - \mathbb{E}_{X, t} \left[ \frac{X_{i,t}^\top X_{i+1,t}}{\|X_{i,t}\|_2 \|X_{i+1,t}\|_2} \right]
	\label{eq:bi}
\end{equation}

در این رابطه‌ی\ref{eq:bi} \( X_{i,t} \) نشان‌دهنده \( t \)-امین ردیف از حالات پنهان لایه \( i \) است. \( X_{i+1,t} \) نشان‌دهنده \( t \)-امین ردیف از حالات پنهان لایه \( i+1 \) است. \( \|\cdot\|_2 \) نُرم اقلیدسی را نشان می‌دهد و \( \mathbb{E}_{X, t} \) بیانگر امید ریاضی روی مقادیر \( X \) و \( t \) است. این متریک بر اساس شباهت کسینوسی بین ورودی و خروجی لایه عمل می‌کند. هرچه این شباهت بیشتر باشد (مقدار \( BI \) کمتر)، لایه تغییرات کمتری ایجاد کرده و اهمیت آن کاهش می‌یابد.


یکی از پژوهش‌های برجسته در زمینه کاهش پیچیدگی مدل‌های زبانی بزرگ، مقاله‌ای از یانگ ژانگ است که روش برش دقیق‌تر \LTRfootnote{FINERCUT} را برای هرس لایه‌های مدل‌های زبانی بزرگ ارائه می‌دهد. این روش برخلاف روش‌های پیشین، لایه‌های خودتوجهی\LTRfootnote{self-attention} و شبکه عصبی پیشخور\LTRfootnote{FFN} را به‌صورت جداگانه و مستقل به‌عنوان کاندیداهای هرس در نظر می‌گیرد. الگوریتم به‌صورت تکراری لایه‌هایی را که حذف آن‌ها کمترین تغییر را در خروجی مدل ایجاد می‌کنند، انتخاب و حذف می‌کند. برای اندازه‌گیری تغییرات خروجی، از معیارهای مختلفی مانند فاصله اقلیدسی، فاصله زاویه‌ای و واگرایی جنسن-شانون ($JSD$) استفاده می‌شود. معیار $JSD$، که برای سنجش شباهت بین توزیع‌های احتمالاتی طراحی شده، با در نظر گرفتن توزیع خروجی‌های مدل قبل و بعد از حذف یک لایه، به شناسایی لایه‌های کم‌اهمیت کمک می‌کند.

نتایج این روش نشان می‌دهد که با حذف ۲۵٪ از لایه‌های لاما 3-8$B$، ۹۰٪ از عملکرد مدل حفظ می‌شود و با حذف ۳۰٪ از لایه‌های لاما 3-70$B$، مدل توانسته است ۹۵٪ عملکرد اولیه را حفظ کند، بدون نیاز به تنظیم دوباره. همچنین، در لایه‌های انتهایی مدل، ترکیبی از حذف لایه‌های توجه و استفاده از لایه‌های متوالی شبکه تغذیه پیش‌رو، به ساختارهای کارآمدتری منجر شده است. این روش نه تنها عملکرد بهتری نسبت به روش‌های پیشرو ارائه می‌دهد، بلکه طراحی جدیدی برای معماری مدل‌های آینده پیشنهاد می‌کند که می‌تواند بازدهی بیشتری در استفاده از منابع محاسباتی داشته باشد\cite{zhang2024finercut}.



\subsection{هزیان‌گویی در خلاصه‌سازی}
یکی از مطالعات برجسته در زمینه کاهش هزیان‌گویی در مدل‌های زبانی بزرگ  و بهبود کیفیت خلاصه‌سازی انتزاعی متون طولانی، مقاله‌ای از یو شیا است. این تحقیق اولین چارچوب یادگیری فعال برای کاهش هزیان‌گویی در مدل‌های زبانی ارائه می‌دهد و بر روی تولید خلاصه‌های متنی با تأکید بر حفظ دقت معنایی متمرکز است. روش پیشنهادی، سه نوع خطای رایج شامل خطاهای چارچوب معنایی، خطاهای گفتمان، و خطاهای قابلیت تأیید محتوا را شناسایی کرده و از متریک‌های پیشرفته‌ای مانند $FactKB$، $UniEval$ و$ BERT-P$ برای ارزیابی این خطاها استفاده می‌کند.

الگوریتم $HADAS$، با رویکردی داده‌محور و متنوع‌محور، نمونه‌های داده‌ای را که شامل انواع مختلفی از هزیان‌گوییات هستند برای بهبود مدل انتخاب می‌کند. این انتخاب با استفاده از واگرایی جنسن-شانون ($JSD$) به منظور سنجش تنوع توزیع انواع هزیان‌گوییات انجام می‌شود. این روش نه تنها به طور مؤثری هزیان‌گوییات در خلاصه‌سازی‌های انتزاعی را کاهش داده است، بلکه کیفیت و دقت خلاصه‌های تولیدشده را نیز بهبود می‌بخشد. نتایج نشان می‌دهد که این چارچوب، با کاهش نیاز به حاشیه‌نویسی انسانی پرهزینه و ارائه روشی کارآمد برای هرس مدل، به‌طور خاص در زمینه خلاصه‌سازی متون طولانی تأثیرگذار است\cite{xia2024hallucinationdiversityawareactivelearning}.


 تایجی لی و همکارانش روش $SliSum$ را برای کاهش هزیان‌گویی و افزایش دقت در خلاصه‌سازی مدل‌های زبان بزرگ ارائه داده‌اند. این روش شامل سه مرحله کلیدی است:
\begin{itemize}
	\item 
تولید خلاصه‌های محلی با پنجره‌های لغزنده: متن منبع به بخش‌های همپوشانی‌شده تقسیم می‌شود (پنجره‌های لغزنده)، و مدل زبان برای هر پنجره یک خلاصه محلی تولید می‌کند. این همپوشانی‌ها کمک می‌کنند که متن به طور عادلانه در سراسر مقاله پردازش شود و مشکل سوگیری موقعیتی کاهش یابد.
\item 
خوشه‌بندی و فیلتر کردن بر اساس انسجام درونی: جملات تولیدشده در خلاصه‌های محلی با استفاده از الگوریتم خوشه‌بندی $Lexical$ (مانند $DBSCAN$) تجزیه و تحلیل می‌شوند. جملات مرتبط با یک رویداد مشخص در یک خوشه قرار می‌گیرند و جملات کم‌اهمیت یا ناسازگار حذف می‌شوند تا انسجام و دقت حفظ شود.
\item 
تجمیع و تولید خلاصه جامع: جملات منتخب از خوشه‌ها، با استفاده از رأی‌گیری اکثریتی و به ترتیب معنایی ترکیب می‌شوند تا یک خلاصه جامع و دقیق برای کل مقاله تولید شود. این مرحله همچنین از مدل زبان برای اطمینان از روانی و ساختارمند بودن متن استفاده می‌کند.
\end{itemize}
این رویکرد، بدون نیاز به تنظیم مجدد مدل یا منابع اضافی، باعث کاهش هزیان‌گویی و افزایش دقت و انسجام خلاصه‌های تولیدشده می‌شود و در متون کوتاه و بلند عملکردی مؤثر نشان می‌دهد. تصویر \ref{fig:slimsum} نشان‌دهنده مراحل مختلف این فرآیند است که شامل پردازش هر پنجره و ترکیب نتایج به روش ساختاریافته است\cite{li-etal-2024-improving-faithfulness}.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[height=5cm]{slimsum.png}
	\end{center}
	\caption{  رویکرد $SlimSum$ برای حل تضادهای معنایی در خلاصه‌سازی }
	\small{
		
		توضیح: تصویری از فرآیند $SlimSum$ که با رأی‌گیری اکثریت میان جملات هر خوشه بر اساس معنای آن‌ها، به حل مشکل تضاد معنایی در خلاصه‌سازی می‌پردازد. به عنوان مثال، جملات سبز دارای معنای مشابه هستند و دو بار ظاهر می‌شوند، در حالی که جمله قرمز با معنای متفاوت فقط یک بار ظاهر می‌شود. بنابراین، جمله دوم سبز برای خلاصه نهایی انتخاب می‌شود. $SlimSum$ مقالات منبع را در سطح جملات پردازش می‌کند و برای ساده‌تر کردن نمایش، پنجره‌های موجود در تصویر به صورت خطوط متنی نمایش داده شده‌اند\cite{li-etal-2024-improving-faithfulness}. }
	\label{fig:slimsum}
	
	\medskip
	
\end{figure}




\section{معیارهای ارزیابی خلاصه‌سازی خودکار}
در ارزیابی خلاصه‌سازی خودکار، هدف اصلی سنجش کیفیت و دقت خلاصه‌های تولید شده است. برای این منظور، معیارهای مختلفی طراحی شده‌اند که توانایی مدل‌ها در تولید خلاصه‌های دقیق، مفهومی و وفادار به متن اصلی را ارزیابی می‌کنند. این معیارها می‌توانند به صورت کمی و بر اساس مقایسه خلاصه‌های تولید شده با متن‌های مرجع، یا به‌طور کیفی با استفاده از تحلیل‌های معنایی و ساختاری عمل کنند. در این بخش، به معرفی و بررسی مهم‌ترین معیارهای ارزیابی در این حوزه مانند روژ، امتیاز برت ، فکت‌سی‌سی پرداخته می‌شود. این معیارها هرکدام از جنبه‌های مختلف کیفیت خلاصه‌ها را ارزیابی کرده و نقش مهمی در توسعه و بهبود الگوریتم‌های خلاصه‌سازی خودکار ایفا می‌کنند. همچنین، محدودیت‌های این معیارها در شناسایی هالوسینیشن و چالش‌های آن‌ها در سنجش وفاداری خلاصه‌ها نیز مورد بررسی قرار خواهد گرفت.
\begin{itemize}
	\item روژ
	 یکی از پرکاربردترین معیارهای ارزیابی در خلاصه‌سازی خودکار است که بر اساس تطابق کلمات یا عبارات $n-gram$ بین خلاصه تولیدشده و متن مرجع عمل می‌کند. روژ شاخص‌هایی مانند دقت \LTRfootnote{Precision} ، فراخوان \LTRfootnote{Recall} و $F1 $را برای شباهت زبانی محاسبه می‌کند. اگرچه این معیار در اندازه‌گیری شباهت‌های سطح کلمه مؤثر است، اما از درک معنایی عمیق و سنجش وفاداری محتوا ناتوان است. به عنوان مثال، $Zhou$ و همکاران نشان داده‌اند که اگر یک خلاصه شامل مقدار زیادی محتوای هالوسینیشن باشد، ممکن است همچنان روژ بالایی کسب کند.\cite{zhou-etal-2021-detecting,lin-2004-rouge}.
	\item 
	امتیاز برت برخلاف معیارهای سطح کلمه مانند روژ، از مدل‌های زبانی پیش‌آموزش‌دیده (مانند برت) برای اندازه‌گیری شباهت معنایی میان خلاصه تولیدشده و متن مرجع استفاده می‌کند. این معیار توانایی بیشتری در درک روابط زبانی پیچیده و شباهت معنایی عمیق دارد، اما همچنان در شناسایی دقیق هالوسینیشن‌ها محدودیت‌هایی دارد\cite{zhang-etal-2024-benchmarking}.
	\item 
	فکت‌سی‌سی مبتنی بر مدل‌های استنتاج متنی\LTRfootnote{Natural Language Inference(NLI)} طراحی شده و تمرکز آن بر بررسی میزان درستی و وفاداری اطلاعات موجود در خلاصه به متن اصلی است. فکت‌سی‌سی تلاش می‌کند تا محتواهای نادرست یا ناسازگار را شناسایی کند و از این طریق بهبودهایی در سنجش وفاداری ایجاد کند. این معیار نسبت به روژ و امتیازبرت توانایی بهتری در ارزیابی هالوسینیشن دارد\cite{factcc-etal-2020-evaluating}.
	
	\item 
	سطح موجودیت\LTRfootnote{Entity-level Metrics}
	بر شناسایی ناسازگاری‌های موجودیتی تمرکز دارند. برای مثال، شناسایی موجودیت‌هایی که در خلاصه ظاهر می‌شوند اما در متن اصلی نیستند (هذیان موجودیتی). این روش  از ابزارهای شناسایی موجودیت نام‌دار\LTRfootnote{NER} استفاده می‌کند\cite{nan_entity-level_2021}.

	\begin{equation}
		\text{دقت موجودیت} = \frac{\text{موجودیت‌ها در فرضیه} \cap \text{موجودیت‌ها در مرجع}}{\text{موجودیت‌ها در فرضیه}}
	\end{equation}
	
	
	
	\item {
	ریسک هزیان \LTRfootnote{HaRiM+}
	به منظور ارزیابی ریسک هذیان در متن‌های تولیدشده توسط مدل‌های خلاصه‌سازی انتزاعی طراحی شده است. این متریک از احتمالات سطح توکن که توسط دو مدل پیش‌بینی می‌شوند، استفاده می‌کند: مدل دنباله به دنباله که به ورودی متن وابسته است و مدل زبان کمکی که مستقل از متن ورودی عمل می‌کند. فرمول متریک به گونه‌ای است که توکن‌هایی با احتمال پایین از مدل دنباله به دنباله و احتمال بالاتر از مدل زبان کمکی را که احتمالاً ناشی از توجه بیش‌ازحد به محتوای قبلی هستند، شناسایی و جریمه می‌کند. ریسک هزیان نیازی به خلاصه‌های مرجع ندارد و به‌صورت مستقیم از مدل‌های موجود بدون نیاز به آموزش اضافه قابل استفاده است. این متریک بر اساس ارزیابی‌های انجام‌شده، ارتباط بالایی با قضاوت انسانی در معیارهایی همچون صحت و کیفیت خلاصه‌ها دارد و ابزاری مؤثر برای ارزیابی کیفیت و کاهش محتوای هذیانی در مدل‌ها ارائه می‌دهد. 
	متریک ریسک هزیان  به صورت ریاضی به شکل زیر تعریف می‌شود:
	

\begin{equation}
	\text{$HaRiM$} = \frac{1}{L} \sum_{i=0}^{L} \left(1 - p_{{s2s}}(y_i | y_{<i}, X)\right) \left(1 - \left(p_{{s2s}}(y_i | y_{<i}, X) - p_{{lm}}(y_i | y_{<i})\right)\right)
\end{equation}

در اینجا:
\begin{itemize}
	\item $L$: طول کل دنباله تولید شده است.
	\item $p_{{s2s}}(y_i | y_{<i}, X)$: احتمال توکن $y_i$ که توسط مدل دنباله به دنباله پیش‌بینی شده است، با توجه به ورودی $X$ و توکن‌های قبلی $y_{<i}$.
	\item $p_{{lm}}(y_i | y_{<i})$: احتمال توکن $y_i$ که توسط مدل زبان کمکی پیش‌بینی شده است، بدون توجه به ورودی $X$.
\end{itemize}

این متریک زمانی افزایش می‌یابد که:
\begin{itemize}
	\item مدل دنباله به دنباله اعتماد کمتری به یک توکن داشته باشد (احتمال $p_{{s2s}}$ کوچک باشد).
	\item مدل زبان کمکی اعتماد بیشتری نسبت به مدل دنباله به دنباله به یک توکن داشته باشد (احتمال $p_{{lm}}$ غالب بر $p_{{s2s}}$ باشد).
\end{itemize}

این رفتار مواردی را شناسایی می‌کند که در آن مدل محتوا را بیشتر تحت تأثیر توکن‌های قبلی تولید کرده است و ارتباط کمتری با متن ورودی دارد، که نشان‌دهنده خطر تولید محتوای هذیانی است.
\cite{son_harim_2022}.
}
	
	\item [SUMMAC]{
	
	
	این مدل بر اساس استفاده از مدل‌های استنتاج زبانی (NLI) برای شناسایی ناسازگاری‌های متنی طراحی شده است. SUMMAC اسناد را به جملات تقسیم کرده و با ترکیب امتیازهای جملات، امتیاز کلی را محاسبه می‌کند. این روش دقتی برابر با 74.4درصد در شناسایی ناسازگاری‌ها ارائه می‌دهد​}
	\item {این مدل بر اساس تقویت مدل‌های زبانی با دانش مبتنی بر پایگاه‌های دانش (KB) عمل می‌کند. سه استراتژی برای پیش‌آموزش مدل‌های زبانی شامل Wiki Entities، Evidence Extraction، و Knowledge Walk به کار گرفته می‌شود. این مدل برای ارزیابی دقت در حوزه‌های علمی و خبری بسیار 
		
	مؤثر است\cite{laban_span_2022}.}
	

	
	
	\item[SUMMEDITS] {
	این متریک بر شناسایی ناسازگاری‌ها در خلاصه‌ها با ایجاد مجموعه‌های داده مبتنی بر ویرایش استوار است. $SUMMEDITS$ با توجه به دامنه‌های مختلف، امکان ارزیابی مدل‌ها را با چالش بیشتری فراهم می‌کند و معیار جدیدی برای توانایی استدلال واقعی مدل‌ها ارائه می‌دهد.$SUMMEDITS$ با تمرکز بر شناسایی خطاهای معنایی، به عنوان یک ابزار مؤثر برای ارزیابی توانایی استدلال مدل‌ها در پردازش متن عمل می‌کند. این متریک نه تنها خطاها را شناسایی می‌کند بلکه برای توسعه مدل‌هایی که توانایی استنتاج دقیق‌تری دارند، کاربردی است\cite{laban_summedits_2023}.}

	
	
	
	
\end{itemize}
\subsection{محدودیت‌ها و پیشرفت‌ها}

اگرچه معیارهایی مانند روژ\LTRfootnote{rouge} و امتیازبرت\LTRfootnote{BERTScore} در اندازه‌گیری شباهت زبانی میان خلاصه‌ها و متن مرجع مؤثر هستند، اما توانایی لازم برای شناسایی هالوسینیشن را ندارند. هالوسینیشن به تولید محتوای نامرتبط، ساختگی یا ناسازگار با متن اصلی اشاره دارد که می‌تواند وفاداری خلاصه‌ها را کاهش دهد و اعتماد کاربران به خروجی مدل را به چالش بکشد.


پژوهش‌های اخیر معیارهای پیشرفته‌تری مانند ریسک هزیان و $SUMMEDITS$ را معرفی کرده‌اند که به طور خاص بر شناسایی هالوسینیشن و ناسازگاری‌های معنایی تمرکز دارند.  این متریک‌ها با ارائه ارزیابی‌های جزئی‌تر و دقیق‌تر، امکان بهبود قابل‌توجه در کیفیت و دقت خلاصه‌سازی انتزاعی را فراهم کرده‌اند و نقاط ضعف معیارهای سنتی مانند روژ را پوشش می‌دهند.











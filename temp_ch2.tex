
\section{روش‌های بهبود مدل‌های زبانی بزرگ}
\subsection{روش های افزایش ورودی مدل‌های بزرگ زبانی}

\subsection{روش های فشرده‌سازی مدل‌های بزرگ زبانی}
همان‌طور که مدل‌های زبانی بزرگ به طور مداوم در بهبود عملکرد خود پیشرفت می‌کنند، شاهد رشد چشمگیر اندازه آن‌ها هستیم. این مدل‌ها در حال حاضر حاوی میلیاردها و حتی تریلیون‌ها پارامتر هستند که به آن‌ها امکان می‌دهد تا الگوهای پیچیده‌تری را در زبان شناسایی و تولید کنند. هزینه بالای آموزش یک LLM موجب  استفاده مجدد از مدل‌های پیش‌آموزش‌دیده برای کارهای مختلف شده است. با این حال، اندازه عظیم LLMها، deploy یک مدل از پیش‌آموزش‌دیده را به امری پرهزینه تبدیل می‌کند. بسیاری از این مدل‌ها برای  تولید خروجی نیازمند چندین پردازنده گرافیکی (GPU) هستند. بنابراین، کاهش الزامات محاسباتی این مدل‌ها از طریق تکنیک‌های فشرده‌سازی مدل از اهمیت بالایی برخوردار است.تکنیک‌های فشرده‌سازی مدل به سه دسته: تقطیر دانش 
\LTRfootnote{distillation}
، هرس کردن 
\LTRfootnote{pruning}
و کوانتایزه‌کردن
\LTRfootnote{quantization}
تقسیم می‌شوند.
\subsubsection{تقطیر دانش}
\todo{بازنویسی}
روش‌­های تقطیر دانش به عنوان وسیله‌­ای برای پر کردن شکاف عملکرد بین مدل­‌های زبانی بزرگ و کوچک استفاده می‌شود. این رویکرد شامل استفاده از دانش مدل‌های زبانی بزرگ مانند جمینی

(به عنوان یک استاد) برای تعلیم مدل زبانی منبع باز کوچک­تر (به عنوان دانشجو) می‌­باشد. در این فرآیند دانشجو عملکرد مدل استاد را تقلید می‌­کند. این فرآیند نه تنها نیازمندی‌های محاسباتی و در نتیجه منابع لازم را کاهش می­‌دهد بلکه باعث می‌­شود مدل­‌های زبانی در دسترس‌­تر باشند و محققان بتواند به قابلیت‌­های پیشرفته­‌تر آن دست یابند. کاربرد دیگر تقطیر دانش در فشرده‌­سازی مدل استاد می‌­باشد. به طوری که بتوانیم مدل استاد را بدون کاهش قابل توجه در عملکرد کارآمدتر کنیم. کاربرد سوم آن به عنوان بهبود مدل دانشجو مطرح می­شود.  به طوری که مدل دانشجو از خودش به عنوان استاد برای بهبود عملکردش استفاده می­کند.
\subsubsection{کوانتیزاسیون }

کوانتیزاسیون یک تکنیک فشرده سازی مدل است که وزن‌ها و فعال‌سازی‌ها را در یک مدل زبان بزرگ از یک نمایش داده با دقت بالا به یک نمایش داده با دقت پایین‌تر تبدیل می‌کند. به عبارت دیگر، از یک نوع داده‌ای که می‌تواند اطلاعات بیشتری را در خود نگه دارد به نوعی که اطلاعات کمتری را در خود نگه می‌دارد. یک مثال معمول از این تبدیل، تبدیل داده‌ها از یک عدد شناور 32 بیتی 
\LTRfootnote{FP32}
به یک عدد صحیح 8 بیتی
\LTRfootnote{Int8}
یا 4 بیتی
\LTRfootnote{Int4}
است.

\subsubsection{هرس کردن}
هرس شبکه‌های عصبی عمیق روشی است برای کاهش پیچیدگی و اندازه مدل‌های یادگیری عمیق بدون کاهش چشمگیر دقت آن‌ها. در این روش، پارامترهای غیرضروری شبکه (وزن‌ها و گاهی بایاس‌ها) حذف می‌شوند. این کار منجر به مدل‌های کوچکتر، سریع‌تر و بهینه‌تر می‌شود که برای اجرا روی دستگاه‌های با منابع محدود بسیار مناسب هستند.

دو رویکرد اصلی برای هرس شبکه‌های عصبی وجود دارد:
\begin{itemize}
	\item{
		هرس بدون ساختار: در این روش، وزن‌های کم‌اهمیت به صورت جداگانه از شبکه حذف می‌شوند. این روش به دلیل حفظ ساختار کلی شبکه، معمولاً دقت مدل را بهتر حفظ می‌کند اما ممکن است منجر به شبکه‌هایی با ساختار نامنظم و پراکنده شود.}
	\item{
		هرس ساختاریافته: در این روش، کل ساختارهایی مانند فیلترها، لایه‌ها یا نرون‌ها از شبکه حذف می‌شوند. این روش می‌تواند منجر به کاهش اندازه مدل به صورت چشمگیر شود اما ممکن است به دقت مدل آسیب بیشتری وارد کند. } 
\end{itemize}
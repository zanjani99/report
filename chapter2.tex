\chapter{معرفی مفاهیم}


\section{خلاصه‌سازی}
خلاصه‌سازی متن به‌عنوان یکی از وظایف کلیدی در حوزه پردازش زبان طبیعی، نقش مهمی در مدیریت و تحلیل داده‌های متنی ایفا می‌کند. این فرآیند شامل فشرده‌سازی اطلاعات موجود در اسناد طولانی به شکلی مختصر و هدفمند است که معنای اصلی متن حفظ شود. در این راستا، استفاده از مدل‌های از پیش آموزش‌دیده در خلاصه‌سازی انتزاعی به دلیل توانایی آن‌ها در تولید متون خلاصه روان و دقیق، به‌طور فزاینده‌ای مورد توجه قرار گرفته است. این مدل‌ها با ترکیب تکنیک‌های پیشرفته یادگیری عمیق و دانش قبلی، امکان استخراج اطلاعات کلیدی را از اسناد حجیم فراهم می‌کنند و ابزار مؤثری برای کاربردهای متنوعی مانند جستجوی اطلاعات و تحلیل محتوا ارائه می‌دهند.
این فرآیند به دلیل تنوع در کاربردها و نیازهای مختلف، شامل رویکردها و طبقه‌بندی‌های متنوعی می‌شود. از جمله این طبقه‌بندی‌ها می‌توان به خلاصه‌سازی تک‌سندی و چندسندی اشاره کرد؛ در خلاصه‌سازی تک‌سندی بر یک متن واحد تمرکز می‌شود و در خلاصه‌سازی چندسندی اطلاعات مرتبط از چند سند ترکیب می‌شود. همچنین، خلاصه‌سازی می‌تواند به صورت استخراجی، انتزاعی یا ترکیبی انجام شود. رویکرد استخراجی جملاتی از متن اصلی انتخاب می‌کند، در حالی که رویکرد انتزاعی، با تولید جملات جدید، ایده‌های اصلی را منتقل می‌کند. علاوه بر این، خلاصه‌ها می‌توانند از نظر زبانی تک‌زبانه، چندزبانه یا بین‌زبانه باشند. روش‌های خلاصه‌سازی نیز به دو دسته نظارت‌شده و نظارت‌نشده تقسیم می‌شوند که هر یک با توجه به نوع داده‌ها و اهداف مختلف، عملکرد و محدودیت‌های خاص خود را دارند.

%خلاصه‌سازی متن، یک وظیفه مهم در پردازش زبان طبیعی است که شامل فشرده سازی مقدار زیادی متن به یک نسخه کوتاه‌تر و مختصرتر با حفظ معنای اصلی آن می‌شود. تکنیک‌های مختلفی برای دستیابی به این هدف توسعه یافته‌اند که هر کدام نقاط قوت و محدودیت‌های خاص خود را دارند. انتخاب تکنیک مناسب به عوامل مختلفی مانند اندازه متن ورودی، قالب خروجی مورد نظر و هدف استفاده از خلاصه‌بستگی دارد.یکی از طبقه‌بندی‌های اصلی بر اساس تعداد اسناد ورودی است: خلاصه‌سازی تک‌سندی و خلاصه‌سازی چندسندی . SDS بر خلاصه‌سازی یک متن واحد تمرکز دارد، در حالی که MDS اطلاعات را از چندین سند مرتبط ترکیب می‌کند. طبقه‌بندی دیگر رویکرد خلاصه‌سازی را در نظر می‌گیرد: استخراج، انتزاعی یا ترکیبی. روش‌های استخراج جمله‌هایی را از متن اصلی انتخاب می‌کنند، در حالی که روش‌های انتزاعی جملات جدیدی تولید می‌کنند که ایده‌های اصلی را منتقل می‌کنند. روش‌های ترکیبی هر دو رویکرد را ترکیب می‌کنند. علاوه بر این، خلاصه‌ها می‌توانند تک‌زبانه، چندزبانه یا بین‌زبانه باشند.

   % الگوریتم‌های خلاصه‌سازی متن به دو دسته اصلی نظارت‌شده و نظارت‌نشده تقسیم می‌شوند. روش‌های نظارت‌شده برای یادگیری، به مجموعه داده‌های آموزشی برچسب‌گذاری‌شده نیاز دارند، در حالی که روش‌های نظارت‌نشده بدون نیاز به داده‌های آموزشی، بر الگوهای آماری متن متکی هستند.
%از نظر محتوایی، خلاصه‌ها می‌توانند به دو دسته نشان‌دهنده و آموزنده تقسیم شوند. خلاصه‌های نشان‌دهنده، تصویری کلی از متن اصلی ارائه می‌دهند و به خواننده کمک می‌کنند تا تصمیم بگیرد که آیا متن کامل را مطالعه کند یا خیر. در مقابل، خلاصه‌های آموزنده، اطلاعات دقیق و مهم متن را به طور خلاصه‌ارائه می‌دهند.طول و سطح جزئیات خلاصه‌نیز می‌تواند متغیر باشد. خلاصه‌ها می‌توانند به صورت تیترهای کوتاه، جملات خلاصه، یا حتی پاراگراف‌های کامل ارائه شوند. انتخاب نوع خلاصه‌به هدف استفاده از آن بستگی دارد.
\cite{ELKASSAS2021113679,}
%\subsection{خلاصه‌سازی استخراجی}


%\subsection{خلاصه‌سازی انتزاعی}





\section{شبکه عصبی}
%شبکه‌های عصبی الگوریتم‌های محاسباتی الهام گرفته از ساختار مغز انسان هستند که برای یادگیری الگوها در داده‌ها به کار می‌روند. این شبکه‌ها از واحدهای پردازشی ساده‌ای به نام نورون تشکیل شده‌اند که با یکدیگر ارتباط برقرار می‌کنند تا اطلاعات را پردازش کنند. شبکه‌های عصبی در طیف گسترده‌ای از کاربردها از جمله تشخیص تصویر، پردازش زبان طبیعی و پیش‌بینی سری‌های زمانی مورد استفاده قرار می‌گیرند.
شبکه‌های عصبی یکی از ابزارهای کلیدی در یادگیری ماشین هستند که بر اساس ساختار و عملکرد مغز انسان طراحی شده‌اند. این شبکه‌ها از لایه‌هایی متشکل از نورون‌های مصنوعی تشکیل می‌شوند که با اتصال‌های وزنی به یکدیگر مرتبط شده‌اند و داده‌ها را به صورت سلسله‌مراتبی پردازش می‌کنند. هر نورون اطلاعاتی را از نورون‌های لایه قبلی دریافت کرده، آن را پردازش می‌کند و به نورون‌های لایه بعدی منتقل می‌سازد. این فرآیند باعث می‌شود شبکه‌های عصبی بتوانند ویژگی‌های پیچیده داده‌ها را استخراج کرده و روابط غیرخطی میان متغیرها را یاد بگیرند. با استفاده از الگوریتم‌های بهینه‌سازی مانند پس‌انتشار خطا، این شبکه‌ها به‌طور مداوم تنظیم شده و دقت خود را در انجام وظایف مختلف مانند طبقه‌بندی، رگرسیون و یادگیری عمیق بهبود می‌دهند.
%\subsection{شبکه عصبی دنباله به دنباله}
\subsection{ترنسفورمر}
 %مدل ترنسفورمر نوعی معماری شبکه عصبی است که از مکانیزم خود-توجهی استفاده می‌کند تا اهمیت قسمت‌های مختلف یک دنباله ورودی را وزن‌دهی کند و بتواند وابستگی‌های بلندمدت را درک کند و در بسیاری از تسک ‌ها کارآمد باشد. با این حال، پردازش متون طولانی چالش‌های منحصر به فردی را برای مدل‌های مبتنی بر ترنسفورمر ایجاد می‌کند. محدودیت‌های ذاتی طول مدل‌های از پیش آموزش‌داده شده (PLM) اغلب متون طولانی را کوتاه می‌کنند، که منجر به از دست رفتن اطلاعات می‌شود. علاوه بر این، پیچیدگی محاسباتی مدل‌سازی دنباله‌های طولانی می‌تواند برای کاربردهای عملی محدودکننده باشد. برای رفع این مشکلات، تکنیک‌های نوآورانه‌ای لازم است تا متون طولانی را به طور مؤثر پیش‌پردازش کنیم، کارایی محاسباتی را بهینه کنیم و ساختارهای سلسله‌مراتبی پیچیده و الگوهای گفتاری موجود در چنین اسنادی را درک کنیم
مدل ترنسفورمر یکی از معماری‌های پیشرفته شبکه‌های عصبی است که از مکانیزم خود-توجهی
   \LTRfootnote{Self-Attention}  
   بهره می‌برد تا به‌طور مؤثر وابستگی‌های موجود در دنباله‌های ورودی را شناسایی و وزن‌دهی کند. این مدل به‌ویژه در پردازش زبان طبیعی و تسک‌های پیچیده‌ای مانند ترجمه ماشینی، خلاصه‌سازی و تحلیل احساسات توانسته است موفقیت‌های چشمگیری کسب کند. در حالی که ترنسفورمرها در درک وابستگی‌های بلندمدت و ارتباطات پیچیده میان کلمات و جملات بسیار کارآمد هستند، پردازش متون طولانی برای این مدل‌ها چالش‌های خاص خود را به همراه دارد. یکی از این چالش‌ها محدودیت‌های ذاتی در اندازه ورودی است که مدل‌های از پیش آموزش‌داده‌شده (PLM) را مجبور می‌کند تا متون طولانی را به تکه‌های کوچکتر تقسیم کنند، که این امر ممکن است منجر به از دست رفتن اطلاعات مهم شود. علاوه بر این، پیچیدگی محاسباتی در پردازش دنباله‌های طولانی می‌تواند در کاربردهای عملی باعث محدودیت‌هایی شود. برای مقابله با این مسائل، نیاز به تکنیک‌های نوآورانه است که بتوانند پردازش متون طولانی را بهینه کرده و ساختارهای پیچیده‌تر و الگوهای زبان‌شناختی موجود در این متون را به‌طور مؤثر مدل‌سازی کنند.
 \cite{dong2023surveylongtextmodeling} .





\subsection{ مدل‌های زبانی از پیش آموزش دیده و مدل‌های بزرگ زبانی}
در سال‌های اخیر، مدل‌های زبانی پیش‌آموزش‌دیده (PLM) و مدل‌های بزرگ زبانی (LLM) پیشرفت‌های چشمگیری در زمینه پردازش زبان طبیعی داشته‌اند. مدل‌های PLM ابتدا بر روی مجموعه‌داده‌های عظیم آموزش می‌بینند و سپس برای انجام وظایف مختلف زبان طبیعی مانند تولید متن، تحلیل احساسات و ترجمه ماشینی تنظیم می‌شوند. این مدل‌ها توانایی استخراج الگوهای پیچیده زبانی را از داده‌ها دارند و به‌طور مؤثر برای بسیاری از وظایف کاربردی پردازش زبان طبیعی استفاده می‌شوند. با ظهور معماری ترانسفورمر، مدل‌های PLM به‌طور قابل‌توجهی بهبود یافته و قادر به پردازش وابستگی‌های بلندمدت در متون شده‌اند و عملکرد بهتری در بسیاری از کاربردها از خود نشان می‌دهند.

از سوی دیگر، مدل‌های بزرگ زبانی (LLM) مشابه مدل‌های PLM هستند، اما دارای ویژگی‌هایی متفاوت در پردازش اطلاعات هستند. یکی از ویژگی‌های اصلی این مدل‌ها، قابلیت پردازش دامنه وسیع‌تری از متن در یک بار اجرا است که به آن‌ها اجازه می‌دهد اسناد طولانی‌تر را به‌طور مؤثرتر پردازش کنند. این ویژگی به‌ویژه برای وظایفی مانند خلاصه‌سازی متون طولانی بسیار مفید است. به‌علاوه، LLMها توانایی تعمیم‌پذیری بالایی دارند و می‌توانند حتی با تعداد محدودی نمونه، عملکرد خوبی در بسیاری از وظایف پردازش زبان طبیعی داشته باشند.

با این حال، مدل‌های LLM همچنان با محدودیت‌هایی مواجه هستند، از جمله محدودیت در حداکثر طول ورودی و تعداد توکن‌هایی که می‌توانند در یک بار پردازش شوند. به همین دلیل، تحقیق و توسعه روش‌های کارآمد برای پردازش و خلاصه‌سازی متون طولانی با استفاده از این مدل‌ها، همچنان یک چالش تحقیقاتی فعال است.
 \cite{noauthor_better_nodate}\cite{dong2023surveylongtextmodeling} \cite{Li2021PretrainedLM,}.
 \subsection{روش های فشرده‌سازی مدل‌های بزرگ زبانی}
 با توجه به پیشرفت‌های سریع در مدل‌های زبانی بزرگ (LLM)، شاهد رشد چشمگیر اندازه این مدل‌ها در سال‌های اخیر هستیم. این مدل‌ها که دارای میلیاردها و حتی تریلیون‌ها پارامتر هستند، قادر به شناسایی و تولید الگوهای پیچیده در زبان طبیعی می‌باشند. با این حال، این اندازه بزرگ موجب ایجاد چالش‌های عمده‌ای در آموزش و استقرار مدل‌ها می‌شود، زیرا برای آموزش و استفاده از این مدل‌ها به منابع محاسباتی عظیم، مانند پردازنده‌های گرافیکی متعدد، نیاز است. در این راستا، فشرده‌سازی مدل‌ها به‌عنوان یک راهکار مؤثر برای کاهش این نیازهای محاسباتی و بهبود کارایی مدل‌ها مورد توجه قرار گرفته است. تکنیک‌های فشرده‌سازی مدل‌های بزرگ به سه دسته اصلی تقسیم می‌شوند: تقطیر دانش، هرس کردن و کوانتایزاسیون.
 \subsection{تقطیر دانش}
 
 تقطیر دانش روشی است که به‌منظور پر کردن شکاف عملکرد بین مدل‌های بزرگ و مدل‌های کوچک‌تر استفاده می‌شود. در این فرآیند، مدل بزرگ‌تر (مدل استاد) به‌عنوان مرجعی برای آموزش یک مدل کوچک‌تر (مدل دانشجو) عمل می‌کند. مدل دانشجو سعی می‌کند تا عملکرد مدل استاد را تقلید کند. این روش علاوه بر کاهش نیازهای محاسباتی، دسترسی به مدل‌های پیشرفته‌تر را برای محققان تسهیل می‌کند و در عین حال موجب می‌شود که مدل‌های کوچک‌تر بتوانند به‌طور مؤثر و با منابع محدود‌تر، وظایف مشابه مدل‌های بزرگ را انجام دهند. تقطیر دانش همچنین در بهینه‌سازی مدل‌های بزرگ‌تر نیز کاربرد دارد، به‌طوری که مدل‌های استاد می‌توانند بدون کاهش عملکرد قابل‌توجه، به‌طور کارآمدتر اجرا شوند.
 \subsection{کوانتایزاسیون}
 
 کوانتیزاسیون یک تکنیک فشرده‌سازی است که در آن دقت داده‌های ورودی مدل کاهش می‌یابد. در این فرآیند، وزن‌ها و فعالیت‌های مدل‌های بزرگ زبانی از یک نوع داده با دقت بالا (مثلاً 32 بیت) به نوع داده‌ای با دقت پایین‌تر (مانند 8 بیت یا 4 بیت) تبدیل می‌شوند. این کاهش دقت به مدل این امکان را می‌دهد که سریع‌تر و با نیاز به منابع محاسباتی کمتری اجرا شود، در حالی که تقریباً همان عملکرد را در پردازش داده‌ها حفظ می‌کند. این تکنیک به‌ویژه در استقرار مدل‌ها در دستگاه‌های با منابع محدود یا در پردازش‌های زمان واقعی بسیار مفید است.
\subsection{ هرس کردن}
 
 هرس کردن به‌عنوان روشی برای کاهش پیچیدگی و اندازه مدل‌های یادگیری عمیق استفاده می‌شود. در این روش، وزن‌ها و پارامترهای کم‌اهمیت مدل از شبکه حذف می‌شوند، بدون آنکه دقت مدل به‌شدت کاهش یابد. این امر منجر به مدل‌های کوچکتر و سریع‌تر می‌شود که برای اجرا در سیستم‌های با منابع محدود یا در پردازش‌های آنلاین بسیار مناسب است. هرس کردن به دو دسته اصلی تقسیم می‌شود:
 
 هرس بدون ساختار: در این روش، وزن‌های کم‌اهمیت از شبکه حذف می‌شوند، در حالی که ساختار کلی شبکه حفظ می‌شود. این روش به‌طور معمول دقت مدل را به‌خوبی حفظ می‌کند، اما ممکن است منجر به ساختار پراکنده‌تری در شبکه شود.
 هرس ساختاریافته: در این روش، بخش‌های بزرگ‌تری از شبکه مانند لایه‌ها، فیلترها یا نرون‌ها از شبکه حذف می‌شوند. این روش می‌تواند به‌طور چشمگیری اندازه مدل را کاهش دهد، اما ممکن است به دقت مدل آسیب وارد کند.
 
 این تکنیک‌ها به‌طور گسترده‌ای برای بهینه‌سازی مدل‌های بزرگ زبانی و کاهش نیازهای محاسباتی طراحی شده‌اند و می‌توانند بهبود عملکرد و کارایی این مدل‌ها را در زمینه‌های مختلف پردازش زبان طبیعی فراهم کنند.
 
 
 
 

\subsection{ تنظیم دقیق بهینه‌شده پارامترها برای مدل‌های زبانی بزرگ}
تنظیم دقیق کارآمد پارامترها (PEFT) روشی عملی است که به ما اجازه می‌دهد مدل‌های بزرگ زبان را به طور کارآمد برای انجام وظایف مختلف، بدون نیاز به آموزش مجدد کامل آن‌ها، آماده کنیم. این روش با تنظیم دقیق بخشی از پارامترهای یک مدل از پیش آموزش دیده، آن را برای انجام وظایف خاص تطبیق می‌دهد. این کار به طور قابل توجهی هزینه محاسباتی و زمانی مورد نیاز برای آموزش مدل را کاهش می‌دهد. این روش به ویژه برای مدل‌های بسیار بزرگ زبانی که دارای میلیاردها پارامتر هستند بسیار مفید است، زیرا آموزش مجدد کامل این مدل‌ها نیازمند منابع محاسباتی عظیمی است
\cite{Han2024ParameterEfficientFF}.

\subsubsection{لورا}
لورا
\LTRfootnote{LoRA }
روشی برای بهبود کارایی و کاهش هزینه‌های آموزش مدل‌های زبانی بزرگ (LLMs) است. این روش با حفظ ثابت بودن وزن‌های پیش‌آموزشی مدل و جایگزینی ماتریس‌های قابل آموزش با رتبه پایین در هر لایه از معماری ترانسفورمر کار می‌کند. این رویکرد هوشمندانه تعداد پارامترهای قابل آموزش را به طور چشمگیری کاهش می‌دهد، که منجر به نیاز کمتر به حافظه $GPU$ و سرعت آموزش بالاتر می‌شود. $LoRA$ با استفاده از جبر خطی، این بهبود کارایی را بدون افزایش زمان استفاده از مدل و حتی با حفظ کیفیت قابل مقایسه مدل به دست می‌آورد
\cite{hu2021loralowrankadaptationlarge}.

\subsubsection{تنظیم دقیق پرامپت}
تنظیم دقیق پرامپت (Prompt Tuning) یک روش است که در آن پارامترهای اضافی به نام "پرامپت‌های نرم" به ورودی یک مدل زبانی اضافه می‌شود. این پرامپت‌های نرم نحوه تفسیر ورودی توسط مدل را تحت تأثیر قرار می‌دهند و امکان تنظیمات بدون تغییر وزن‌های مدل را فراهم می‌کنند. این روش تعادلی بین بهبود عملکرد و بهره‌وری منابع برقرار می‌کند و در مواردی که منابع محاسباتی محدود هستند یا نیاز به انعطاف‌پذیری در چندین کار وجود دارد، بسیار مفید است. از آنجایی که وزن‌های اصلی مدل بدون تغییر باقی می‌مانند، تنظیم دقیق پرامپت راهی مناسب برای تطبیق رفتار مدل بدون نیاز به بازآموزی گسترده است
\cite{lester2021powerscaleparameterefficientprompt}.



\section{توهم}
توهم یا هالوسینیشن
\LTRfootnote{Hallucination}
  در مدل‌های زبانی بزرگ (LLM) به پدیده‌ای اشاره دارد که در آن مدل محتوایی تولید می‌کند که با داده‌های ورودی نامرتبط، ساختگی یا ناسازگار است. این مشکل می‌تواند به ارائه اطلاعات نادرست منجر شود و اعتماد کاربران به این مدل‌ها را کاهش دهد. در خلاصه‌سازی متون طولانی، این مشکل بیشتر نمایان می‌شود، زیرا مدل‌ها معمولاً تمایل دارند اطلاعات بیشتری را از بخش‌های ابتدایی و انتهایی متن استخراج کنند و به جزئیات میانی متن کمتر توجه داشته باشند. این عدم توازن در پردازش متن می‌تواند باعث تولید اطلاعات نادرست یا بی‌ارتباط با منبع شود. برای مقابله با این چالش، روش‌هایی مانند استفاده از پنجره‌های همپوشانی و تکنیک‌های خودسازگاری (Self-Consistency) پیشنهاد شده‌اند. این رویکردها با تقسیم متن به بخش‌های کوچک‌تر، تولید خلاصه‌های محلی، و سپس ترکیب آن‌ها، به کاهش تناقضات و افزایش دقت خلاصه‌ها کمک می‌کنند. این تکنیک‌ها نه‌تنها دقت و سازگاری خلاصه‌ها را بهبود می‌بخشند، بلکه موجب کاهش خطای هالوسینیشن در کاربردهای عملی می‌شوند.


@article{ELKASSAS2021113679,
	title = {Automatic text summarization: A comprehensive survey},
	journal = {Expert Systems with Applications},
	volume = {165},
	pages = {113679},
	year = {2021},
	issn = {0957-4174},
	doi = {https://doi.org/10.1016/j.eswa.2020.113679},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
	author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
	keywords = {Automatic text summarization, Text summarization approaches, Text summarization techniques, Text summarization evaluation},
	abstract = {Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.}
}

@article{luhn1958automatic,
	title={The automatic creation of literature abstracts},
	author={Luhn, Hans Peter},
	journal={IBM Journal of research and development},
	volume={2},
	number={2},
	pages={159--165},
	year={1958},
	publisher={Ibm}
}
@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}
@article{elman1990finding,
	title={Finding structure in time},
	author={Elman, Jeffrey L},
	journal={Cognitive science},
	volume={14},
	number={2},
	pages={179--211},
	year={1990},
	publisher={Wiley Online Library}
}
@article{hochreiter1997long,
	title={Long short-term memory},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997},
	publisher={MIT press}
}

@inproceedings{zhang2020pegasus,
	title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
	author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
	booktitle={International Conference on Machine Learning},
	pages={11328--11339},
	year={2020},
	organization={PMLR}
}

@article{sherborne2023meta,
	title={Meta-learning a cross-lingual manifold for semantic parsing},
	author={Sherborne, Tom and Lapata, Mirella},
	journal={Transactions of the Association for Computational Linguistics},
	volume={11},
	pages={49--67},
	year={2023},
	publisher={MIT Press}
}

@inproceedings{andhale2016overview,
	title={An overview of text summarization techniques},
	author={ Narendra, Andhale and Bewoor, Laxmi A},
	booktitle={2016 international conference on computing communication control and automation (ICCUBEA)},
	pages={1--7},
	year={2016},
	organization={IEEE}
}
@article{lee2005fuzzy,
	title={A fuzzy ontology and its application to news summarization},
	author={Lee, Chang-Shing and Jian, Zhi-Wei and Huang, Lin-Kai},
	journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	volume={35},
	number={5},
	pages={859--880},
	year={2005},
	publisher={IEEE}
}
@inproceedings{zhang2024pruning,
	title={Pruning as a Domain-specific LLM Extractor},
	author={Zhang, Nan and Liu, Yanchi and Zhao, Xujiang and Cheng, Wei and Bao, Runxue and Zhang, Rui and Mitra, Prasenjit and Chen, Haifeng},
	booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
	pages={1417--1428},
	year={2024}
}
@article{men2024shortgpt,
	title={ShortGPT: Layers in Large Language Models are More Redundant Than You Expect},
	author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
	journal={arXiv e-prints},
	pages={arXiv--2403},
	year={2024}
}
@article{zhang2024finercut,
	title={FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models},
	author={Zhang, Yang and Li, Yawei and Wang, Xinpeng and Shen, Qianli and Plank, Barbara and Bischl, Bernd and Rezaei, Mina and Kawaguchi, Kenji},
	journal={arXiv e-prints},
	pages={arXiv--2405},
	year={2024}
}
@misc{xia2024hallucinationdiversityawareactivelearning,
	title={Hallucination Diversity-Aware Active Learning for Text Summarization}, 
	author={Yu Xia and Xu Liu and Tong Yu and Sungchul Kim and Ryan A. Rossi and Anup Rao and Tung Mai and Shuai Li},
	year={2024},
	eprint={2404.01588},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.01588}, 
}
@inproceedings{li-etal-2024-improving-faithfulness,
	title = "Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency",
	author = "Li, Taiji  and
	Li, Zhi  and
	Zhang, Yin",
	editor = "Calzolari, Nicoletta  and
	Kan, Min-Yen  and
	Hoste, Veronique  and
	Lenci, Alessandro  and
	Sakti, Sakriani  and
	Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.771",
	pages = "8804--8817",
	abstract = "Despite large language models (LLMs) have demonstrated impressive performance in various tasks, they are still suffering from the factual inconsistency problem called hallucinations. For instance, LLMs occasionally generate content that diverges from source article, and prefer to extract information that appears at the beginning and end of the context, especially in long document summarization. Inspired by these findings, we propose to improve the faithfulness of LLMs in summarization by impelling them to process the entire article more fairly and faithfully. We present a novel summary generation strategy, namely SliSum, which exploits the ideas of sliding windows and self-consistency. Specifically, SliSum divides the source article into overlapping windows, and utilizes LLM to generate local summaries for the content in the windows. Finally, SliSum aggregates all local summaries using clustering and majority voting algorithm to produce more faithful summary of entire article. Extensive experiments demonstrate that SliSum significantly improves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and GPT-3.5 in both short and long text summarization, while maintaining their fluency and informativeness and without additional fine-tuning and resources. We further conduct qualitative and quantitative studies to investigate why SliSum works and impacts of hyperparameters in SliSum on performance.",
}


@inproceedings{Ishikawa2001HybridTS,
	title={Hybrid Text Summarization Method based on the TF Method and the Lead Method},
	author={Kai Ishikawa and Shinichi Ando and Akitoshi Okumura},
	booktitle={NTCIR Conference on Evaluation of Information Access Technologies},
	year={2001},
	url={https://api.semanticscholar.org/CorpusID:1619406}
}
@inproceedings{GraphBased,
	author = {Malliaros, Fragkiskos D. and Skianis, Konstantinos},
	title = {Graph-Based Term Weighting for Text Categorization},
	year = {2015},
	isbn = {9781450338547},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2808797.2808872},
	doi = {10.1145/2808797.2808872},
	abstract = {Text categorization is an important task with plenty of applications, ranging from sentiment analysis to automated news classification. In this paper, we introduce a novel graph-based approach for text categorization. Contrary to the traditional Bag-of-Words model for document representation, we consider a model in which each document is represented by a graph that encodes relationships between the different terms. The importance of a term to a document is indicated using graph-theoretic node centrality criteria. The proposed weighting scheme is able to meaningfully capture the relationships between the terms that co-occur in a document, creating feature vectors that can improve the categorization task. We perform experiments in well-known document collections, applying popular classification algorithms. Our preliminary results indicate that the proposed graph-based weighting mechanism is able to outperform existing frequency-based term weighting criteria, under appropriate parameter setting.},
	booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
	pages = {1473–1479},
	numpages = {7},
	location = {Paris, France},
	series = {ASONAM '15}
}

@INPROCEEDINGS{Moratanchsurvey,
	
	author={Moratanch, N. and Chitrakala, S.},
	
	booktitle={2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)}, 
	
	title={A survey on abstractive text summarization}, 
	
	year={2016},
	
	volume={},
	
	number={},
	
	pages={1-7},
	
	doi={10.1109/ICCPCT.2016.7530193}}



@inproceedings{chen2018fast,
	title={Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting},
	author={Chen, Yen-Chun and Bansal, Mohit},
	booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages={675--686},
	year={2018}
}
@article{liu2020survey,
	title={A survey on contextual embeddings},
	author={Liu, Qi and Kusner, Matt J and Blunsom, Phil},
	journal={arXiv preprint arXiv:2003.07278},
	year={2020}
}
@inproceedings{lewis-etal-2020-bart,
	title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
	author = "Lewis, Mike  and
	Liu, Yinhan  and
	Goyal, Naman  and
	Ghazvininejad, Marjan  and
	Mohamed, Abdelrahman  and
	Levy, Omer  and
	Stoyanov, Veselin  and
	Zettlemoyer, Luke",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.703",
	doi = "10.18653/v1/2020.acl-main.703",
	pages = "7871--7880",
	abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}
@article{yao2018dual,
	title={Dual encoding for abstractive text summarization},
	author={Yao, Kaichun and Zhang, Libo and Du, Dawei and Luo, Tiejian and Tao, Lili and Wu, Yanjun},
	journal={IEEE transactions on cybernetics},
	volume={50},
	number={3},
	pages={985--996},
	year={2018},
	publisher={IEEE}
}

@article{Ma2022TBERTSumTT,
	title={T-BERTSum: Topic-Aware Text Summarization Based on BERT},
	author={Tinghuai Ma and Qian Pan and Huan Rong and Yurong Qian and Yuan Tian and Najla Abdulrahman Al-Nabhan},
	journal={IEEE Transactions on Computational Social Systems},
	year={2022},
	volume={9},
	pages={879-890},
	url={https://api.semanticscholar.org/CorpusID:237976609}
}

@inproceedings{variational,
	title = "A Variational Hierarchical Model for Neural Cross-Lingual Summarization",
	author = "Liang, Yunlong  and
	Meng, Fandong  and
	Zhou, Chulun  and
	Xu, Jinan  and
	Chen, Yufeng  and
	Su, Jinsong  and
	Zhou, Jie",
	booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.acl-long.148",
	doi = "10.18653/v1/2022.acl-long.148",
	pages = "2088--2099",
	abstract = "The goal of the cross-lingual summarization (CLS) is to convert a document in one language (e.g., English) to a summary in another one (e.g., Chinese). The CLS task is essentially the combination of machine translation (MT) and monolingual summarization (MS), and thus there exists the hierarchical relationship between MT{\&}MS and CLS. Existing studies on CLS mainly focus on utilizing pipeline methods or jointly training an end-to-end model through an auxiliary MT or MS objective. However, it is very challenging for the model to directly conduct CLS as it requires both the abilities to translate and summarize. To address this issue, we propose a hierarchical model for the CLS task, based on the conditional variational auto-encoder. The hierarchical model contains two kinds of latent variables at the local and global levels, respectively. At the local level, there are two latent variables, one for translation and the other for summarization. As for the global level, there is another latent variable for cross-lingual summarization conditioned on the two local-level variables. Experiments on two language directions (English-Chinese) verify the effectiveness and superiority of the proposed approach. In addition, we show that our model is able to generate better cross-lingual summaries than comparison models in the few-shot setting.",
}

@article{DeepTL_RL,
	title={Deep Transfer Reinforcement Learning for Text Summarization},
	author={Yaser Keneshloo and Naren Ramakrishnan and Chandan K. Reddy},
	journal={ArXiv},
	year={2018},
	volume={abs/1810.06667},
	url={https://api.semanticscholar.org/CorpusID:53116124}
}
@inproceedings{Parnell2022AMC,
	title={A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization},
	author={Jacob Parnell and Inigo Jauregi Unanue and Massimo Piccardi},
	booktitle={Annual Meeting of the Association for Computational Linguistics},
	year={2022},
	url={https://api.semanticscholar.org/CorpusID:247292760}
}
@article{Grathwohl2017BackpropagationTT,
	title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},
	author={Will Grathwohl and Dami Choi and Yuhuai Wu and Geoffrey Roeder and David Kristjanson Duvenaud},
	journal={ArXiv},
	year={2017},
	volume={abs/1711.00123},
	url={https://api.semanticscholar.org/CorpusID:3535369}
}

@inproceedings{reformer,
	title={Reformer: The Efficient Transformer},
	author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
	booktitle={International Conference on Learning Representations},
	year={2019}
}


@article{zaheer2020big,
	title={Big bird: Transformers for longer sequences},
	author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={17283--17297},
	year={2020}
}

@misc{child2019generating,
	title={Generating Long Sequences with Sparse Transformers}, 
	author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
	year={2019},
	eprint={1904.10509},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@inproceedings{xiao2020systematically,
	title={Systematically Exploring Redundancy Reduction in Summarizing Long Documents},
	author={Xiao, Wen and Carenini, Giuseppe},
	booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
	pages={516--528},
	year={2020}
}
@inproceedings{pilault2020extractive,
	title={On extractive and abstractive neural document summarization with transformer language models},
	author={Pilault, Jonathan and Li, Raymond and Subramanian, Sandeep and Pal, Christopher},
	booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	pages={9308--9319},
	year={2020}
}


@article{Xiong2022AdaptingPT,
	title={Adapting Pretrained Text-to-Text Models for Long Text Sequences},
	author={Wenhan Xiong and Anchit Gupta and Shubham Toshniwal and Yashar Mehdad and Wen-tau Yih},
	journal={ArXiv},
	year={2022},
	volume={abs/2209.10052},
	url={https://api.semanticscholar.org/CorpusID:252407634}
}

@ARTICLE{twostage,
	
	author={Su, Ming-Hsiang and Wu, Chung-Hsien and Cheng, Hao-Tse},
	
	journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
	
	title={A Two-Stage Transformer-Based Approach for Variable-Length Abstractive Summarization}, 
	
	year={2020},
	
	volume={28},
	
	number={},
	
	pages={2061-2072},
	
	doi={10.1109/TASLP.2020.3006731}}

@article{RL_survey,
	title = {Deep reinforcement and transfer learning for abstractive text summarization: A review},
	journal = {Computer Speech and Language},
	volume = {71},
	pages = {101276},
	year = {2022},
	issn = {0885-2308},
	doi = {https://doi.org/10.1016/j.csl.2021.101276},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230821000796},
	author = { Norisma Idris and Ayham Alomari and Aznul Qalid Md Sabri and Izzat Alsmadi}
}

@inproceedings{liu2018generative,
	title={Generative adversarial network for abstractive text summarization},
	author={Liu, Linqing and Lu, Yao and Yang, Min and Qu, Qiang and Zhu, Jia and Li, Hongyan},
	booktitle={Proceedings of the AAAI conference on artificial intelligence},
	volume={32},
	number={1},
	year={2018}
}
@inproceedings{pang2023long,
	title={Long Document Summarization with Top-down and Bottom-up Inference},
	author={Pang, Bo and Nijkamp, Erik and Kryscin?ski, Wojciech and Savarese, Silvio and Zhou, Yingbo and Xiong, Caiming},
	booktitle={Findings of the Association for Computational Linguistics: EACL 2023},
	pages={1237--1254},
	year={2023}
}
@inproceedings{fan-etal-2018-controllable,
	title = "Controllable Abstractive Summarization",
	author = "Fan, Angela  and
	Grangier, David  and
	Auli, Michael",
	booktitle = "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W18-2706",
	doi = "10.18653/v1/W18-2706",
	pages = "45--54",
	abstract = "Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With user input, our system can produce high quality summaries that follow user preferences. Without user input, we set the control variables automatically {--} on the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 F1-ROUGE and human evaluation.",
}
@inproceedings{shapira-etal-2021-extending,
	title = "Extending Multi-Document Summarization Evaluation to the Interactive Setting",
	author = "Shapira, Ori  and
	Pasunuru, Ramakanth  and
	Ronen, Hadar  and
	Bansal, Mohit  and
	Amsterdamer, Yael  and
	Dagan, Ido",
	booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.naacl-main.54",
	doi = "10.18653/v1/2021.naacl-main.54",
	pages = "657--677",
	abstract = "Allowing users to interact with multi-document summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for interactive summarization, focusing on expansion-based interaction, which considers the accumulating information along a user session. Our framework includes a procedure of collecting real user sessions, as well as evaluation measures relying on summarization standards, but adapted to reflect interaction. All of our solutions and resources are available publicly as a benchmark, allowing comparison of future developments in interactive summarization, and spurring progress in its methodological evaluation. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose, which will serve as part of our benchmark. Our extensive experimentation and analysis motivate the proposed evaluation framework design and support its viability.",
}

@inproceedings{shapira-etal-2022-interactive,
	title = "Interactive Query-Assisted Summarization via Deep Reinforcement Learning",
	author = "Shapira, Ori  and
	Pasunuru, Ramakanth  and
	Bansal, Mohit  and
	Dagan, Ido  and
	Amsterdamer, Yael",
	booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.naacl-main.184",
	doi = "10.18653/v1/2022.naacl-main.184",
	pages = "2551--2568",
	abstract = "Interactive summarization is a task that facilitates user-guided exploration of information within a document set. While one would like to employ state of the art neural models to improve the quality of interactive summarization, many such technologies cannot ingest the full document set or cannot operate at sufficient speed for interactivity. To that end, we propose two novel deep reinforcement learning models for the task that address, respectively, the subtask of summarizing salient information that adheres to user queries, and the subtask of listing suggested queries to assist users throughout their exploration. In particular, our models allow encoding the interactive session state and history to refrain from redundancy. Together, these models compose a state of the art solution that addresses all of the task requirements. We compare our solution to a recent interactive summarization system, and show through an experimental study involving real users that our models are able to improve informativeness while preserving positive user experience.",
}

@ARTICLE{PoBRL,
	
	author={Su, DiJia and Su, Difei and Mulvey, John M. and Poor, H.Vincent},
	
	journal={IEEE Transactions on Artificial Intelligence}, 
	
	title={Optimizing Multidocument Summarization by Blending Reinforcement Learning Policies}, 
	
	year={2023},
	
	volume={4},
	
	number={3},
	
	pages={416-427},
	
	doi={10.1109/TAI.2022.3201807}}



@inproceedings{kryscinski-etal-2018-improving,
	title = "Improving Abstraction in Text Summarization",
	author = "Kryscinski, Wojciech  and
	Paulus, Romain  and
	Xiong, Caiming  and
	Socher, Richard",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	month = oct # "-" # nov,
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D18-1207",
	doi = "10.18653/v1/D18-1207",
	pages = "1808--1817",
	abstract = "Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.",
}

@misc{dong2023surveylongtextmodeling,
	title={A Survey on Long Text Modeling with Transformers}, 
	author={Zican Dong and Tianyi Tang and Lunyi Li and Wayne Xin Zhao},
	year={2023},
	eprint={2302.14502},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2302.14502}, 
}

@article{Li2021PretrainedLM,
	title={Pretrained Language Models for Text Generation: A Survey},
	author={Junyi Li and Tianyi Tang and Wayne Xin Zhao and Ji-rong Wen},
	journal={ArXiv},
	year={2021},
	volume={abs/2105.10311},
	url={https://api.semanticscholar.org/CorpusID:235125595}
}

@misc{noauthor_better_nodate,
	title = {Better language models and their implications},
	url = {https://openai.com/index/better-language-models/},
	abstract = {We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.},
	language = {en-US},
	urldate = {2024-08-26},
	file = {Snapshot:/home/yumi/snap/zotero-snap/common/Zotero/storage/JMJZXFE6/better-language-models.html:text/html},
}

@article{Han2024ParameterEfficientFF,
	title={Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey},
	author={Zeyu Han and Chao Gao and Jinyang Liu and Jeff Zhang and Sai Qian Zhang},
	journal={ArXiv},
	year={2024},
	volume={abs/2403.14608},
	url={https://api.semanticscholar.org/CorpusID:268553763}
}

@misc{hu2021loralowrankadaptationlarge,
	title={LoRA: Low-Rank Adaptation of Large Language Models}, 
	author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year={2021},
	eprint={2106.09685},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2106.09685}, 
}
@misc{lester2021powerscaleparameterefficientprompt,
	title={The Power of Scale for Parameter-Efficient Prompt Tuning}, 
	author={Brian Lester and Rami Al-Rfou and Noah Constant},
	year={2021},
	eprint={2104.08691},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2104.08691}, 
}

@article{ELKASSAS2021113679,
	title = {Automatic text summarization: A comprehensive survey},
	journal = {Expert Systems with Applications},
	volume = {165},
	pages = {113679},
	year = {2021},
	issn = {0957-4174},
	doi = {https://doi.org/10.1016/j.eswa.2020.113679},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
	author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
	keywords = {Automatic text summarization, Text summarization approaches, Text summarization techniques, Text summarization evaluation},
	abstract = {Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.}
}


@article{Cao2023AWESOMEGM,
	title={AWESOME: GPU Memory-constrained Long Document Summarization using Memory Mechanism and Global Salient Content},
	author={Shuyang Cao and Lu Wang},
	journal={ArXiv},
	year={2023},
	volume={abs/2305.14806},
	url={https://api.semanticscholar.org/CorpusID:258865559}
}

@inproceedings{maynez-etal-2020-faithfulness,
	title = "On Faithfulness and Factuality in Abstractive Summarization",
	author = "Maynez, Joshua  and
	Narayan, Shashi  and
	Bohnet, Bernd  and
	McDonald, Ryan",
	editor = "Jurafsky, Dan  and
	Chai, Joyce  and
	Schluter, Natalie  and
	Tetreault, Joel",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.173",
	doi = "10.18653/v1/2020.acl-main.173",
	pages = "1906--1919",
	abstract = "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
}
@inproceedings{lin-2004-rouge,
	title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
	author = "Lin, Chin-Yew",
	booktitle = "Text Summarization Branches Out",
	month = jul,
	year = "2004",
	address = "Barcelona, Spain",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W04-1013",
	pages = "74--81",
}

@article{zhang-etal-2024-benchmarking,
	title = "Benchmarking Large Language Models for News Summarization",
	author = "Zhang, Tianyi  and
	Ladhak, Faisal  and
	Durmus, Esin  and
	Liang, Percy  and
	McKeown, Kathleen  and
	Hashimoto, Tatsunori B.",
	journal = "Transactions of the Association for Computational Linguistics",
	volume = "12",
	year = "2024",
	address = "Cambridge, MA",
	publisher = "MIT Press",
	url = "https://aclanthology.org/2024.tacl-1.3",
	doi = "10.1162/tacl_a_00632",
	pages = "39--57",
	abstract = "Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM{'}s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.",
}
@inproceedings{factcc-etal-2020-evaluating,
	title = "Evaluating the Factual Consistency of Abstractive Text Summarization",
	author = "Kryscinski, Wojciech  and
	McCann, Bryan  and
	Xiong, Caiming  and
	Socher, Richard",
	editor = "Webber, Bonnie  and
	Cohn, Trevor  and
	He, Yulan  and
	Liu, Yang",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.750",
	doi = "10.18653/v1/2020.emnlp-main.750",
	pages = "9332--9346",
	abstract = "The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at \url{https://github.com/salesforce/factCC}.",
}
@inproceedings{zhou-etal-2021-detecting,
	title = "Detecting Hallucinated Content in Conditional Neural Sequence Generation",
	author = "Zhou, Chunting  and
	Neubig, Graham  and
	Gu, Jiatao  and
	Diab, Mona  and
	Guzm{\'a}n, Francisco  and
	Zettlemoyer, Luke  and
	Ghazvininejad, Marjan",
	editor = "Zong, Chengqing  and
	Xia, Fei  and
	Li, Wenjie  and
	Navigli, Roberto",
	booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.findings-acl.120",
	doi = "10.18653/v1/2021.findings-acl.120",
	pages = "1393--1404",
}
@inproceedings{king-etal-2022-dont,
	title = "Don{'}t Say What You Don{'}t Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search",
	author = "King, Daniel  and
	Shen, Zejiang  and
	Subramani, Nishant  and
	Weld, Daniel S.  and
	Beltagy, Iz  and
	Downey, Doug",
	editor = "Bosselut, Antoine  and
	Chandu, Khyathi  and
	Dhole, Kaustubh  and
	Gangal, Varun  and
	Gehrmann, Sebastian  and
	Jernite, Yacine  and
	Novikova, Jekaterina  and
	Perez-Beltrachini, Laura",
	booktitle = "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates (Hybrid)",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.gem-1.51",
	doi = "10.18653/v1/2022.gem-1.51",
	pages = "555--571",
	abstract = "Abstractive summarization systems today produce fluent and relevant output, but often {``}hallucinate{''} statements not supported by the source text. We analyze the connection between hallucinations and training data, and find evidence that models hallucinate because they train on target summaries that are unsupported by the source. Based on our findings, we present PINOCCHIO, a new decoding method that improves the consistency of a transformer-based abstractive summarizer by constraining beam search to avoid hallucinations. Given the model states and outputs at a given step, PINOCCHIO detects likely model hallucinations based on various measures of attribution to the source text. PINOCCHIO backtracks to find more consistent output, and can opt to produce no summary at all when no consistent generation can be found. In experiments, we find that PINOCCHIO improves the consistency of generation by an average of 67{\%} on two abstractive summarization datasets, without hurting recall.",
}

@inproceedings{xiao-wang-2021-hallucination,
	title = "On Hallucination and Predictive Uncertainty in Conditional Language Generation",
	author = "Xiao, Yijun  and
	Wang, William Yang",
	editor = "Merlo, Paola  and
	Tiedemann, Jorg  and
	Tsarfaty, Reut",
	booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
	month = apr,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.eacl-main.236",
	doi = "10.18653/v1/2021.eacl-main.236",
	pages = "2734--2744",
	abstract = "Despite improvements in performances on different natural language generation tasks, deep neural models are prone to hallucinating facts that are incorrect or nonexistent. Different hypotheses are proposed and examined separately for different tasks, but no systematic explanations are available across these tasks. In this study, we draw connections between hallucinations and predictive uncertainty in conditional language generation. We investigate their relationship in both image captioning and data-to-text generation and propose a simple extension to beam search to reduce hallucination. Our analysis shows that higher predictive uncertainty corresponds to a higher chance of hallucination. Epistemic uncertainty is more indicative of hallucination than aleatoric or total uncertainties. It helps to achieve better results of trading performance in standard metric for less hallucination with the proposed beam search variant.",
}
@inproceedings{ji-etal-2023-towards,
	title = "Towards Mitigating {LLM} Hallucination via Self Reflection",
	author = "Ji, Ziwei  and
	Yu, Tiezheng  and
	Xu, Yan  and
	Lee, Nayeon  and
	Ishii, Etsuko  and
	Fung, Pascale",
	editor = "Bouamor, Houda  and
	Pino, Juan  and
	Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.123",
	doi = "10.18653/v1/2023.findings-emnlp.123",
	pages = "1827--1843",
	abstract = "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of {``}hallucination{''}, where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
}
@inproceedings{maynez-etal-2020-faithfulness,
	title = "On Faithfulness and Factuality in Abstractive Summarization",
	author = "Maynez, Joshua  and
	Narayan, Shashi  and
	Bohnet, Bernd  and
	McDonald, Ryan",
	editor = "Jurafsky, Dan  and
	Chai, Joyce  and
	Schluter, Natalie  and
	Tetreault, Joel",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.173",
	doi = "10.18653/v1/2020.acl-main.173",
	pages = "1906--1919",
	abstract = "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
}
@article{chrysostomou-etal-2024-investigating,
	title = "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization",
	author = "Chrysostomou, George  and
	Zhao, Zhixue  and
	Williams, Miles  and
	Aletras, Nikolaos",
	journal = "Transactions of the Association for Computational Linguistics",
	volume = "12",
	year = "2024",
	address = "Cambridge, MA",
	publisher = "MIT Press",
	url = "https://aclanthology.org/2024.tacl-1.64",
	doi = "10.1162/tacl_a_00695",
	pages = "1163--1181",
	abstract = "Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode reliability and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights, enabling more efficient sparse inference. Pruned models yield downstream task performance comparable to the original, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study across five summarization datasets, two state-of-the-art pruning methods, and five instruction-tuned LLMs. Surprisingly, we find that hallucinations are less prevalent from pruned LLMs than the original models. Our analysis suggests that pruned models tend to depend more on the source document for summary generation. This leads to a higher lexical overlap between the generated summary and the source document, which could be a reason for the reduction in hallucination risk.1",
}

